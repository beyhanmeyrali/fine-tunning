# BÃ¼yÃ¼k Fine-Tuning Devrimi: Yapay ZekanÄ±n En Dramatik AtÄ±lÄ±mlarÄ±nÄ±n Hikayesi

[![Lisans](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3.11+-green.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)
[![AMD ROCm](https://img.shields.io/badge/AMD-ROCm-red.svg)](https://rocm.docs.amd.com/)

> **ğŸ“ OluÅŸturan:** [Beyhan MEYRALI](https://www.linkedin.com/in/beyhanmeyrali/)  
> **ğŸ›ï¸ Optimize edildi:** [GMKtec K11](https://www.gmktec.com/products/amd-ryzen%E2%84%A2-9-8945hs-nucbox-k11) AMD Ryzen 9 8945HS + Radeon 780M iÃ§in  
> **ğŸ“š Ã–ÄŸrenme YolculuÄŸu:** 15 dakikalÄ±k demolardan Ã¼retim daÄŸÄ±tÄ±mÄ±na kadar

*Parlak zihinlerin, imkansÄ±z zorluklarÄ±n ve her ÅŸeyi deÄŸiÅŸtiren tekniklerin hikayesi*

---

## Ã–nsÃ¶z: Ä°mkansÄ±z RÃ¼ya

Bunu hayal edin: 2020 yÄ±lÄ±nda parlak bir fikriniz olan bir araÅŸtÄ±rmacÄ±sÄ±nÄ±z. GPT-3'Ã¼ Ã¶zel gÃ¶reviniz iÃ§in Ã¶zelleÅŸtirmek istiyorsunuzâ€”belki antik dilleri Ã§evirmek ya da daha iyi kod yazmak iÃ§in. Sadece kÃ¼Ã§Ã¼k bir problem var: GPT-3'Ã¼n 175 milyar parametresi var. SÄ±fÄ±rdan eÄŸitmek 12 milyon dolara mal olur ve bir sÃ¼per bilgisayar gerektirir.

MasanÄ±zda duran dizÃ¼stÃ¼ bilgisayarÄ±nÄ±zaâ€”belki de **AMD Ryzen 9 8945HS ve Radeon 780M'li GMKtec K11** gibi mÃ¼tevazÄ± bir makinayaâ€”bakÄ±yorsunuz ve bu saÃ§malÄ±ÄŸa gÃ¼lÃ¼yorsunuz. Bu, oyuncak Ã§ekiÃ§le AltÄ±n KÃ¶prÃ¼'yÃ¼ yeniden inÅŸa etmeye Ã§alÄ±ÅŸmak gibi.

Ama size 2024'te aynÄ± dizÃ¼stÃ¼ bilgisayarÄ±n GPT-3'ten bile gÃ¼Ã§lÃ¼ modelleri fine-tune edebileceÄŸini sÃ¶ylesem? Ä°mkansÄ±z olanÄ±n sadece mÃ¼mkÃ¼n deÄŸil, aynÄ± zamanda *kolay* hale geldiÄŸini sÃ¶ylesem?

Bu, birkaÃ§ parlak araÅŸtÄ±rmacÄ±nÄ±n sadece daÄŸlarÄ± yerinden oynatmakla kalmayÄ±pâ€”bize hiÃ§ yerinden oynatmamÄ±za gerek olmadÄ±ÄŸÄ±nÄ± Ã¶ÄŸrettiÄŸi hikayenin anlatÄ±mÄ±dÄ±r.

---

## BÃ¶lÃ¼m 1: Temel - Microsoft Her Åeyi DeÄŸiÅŸtirdiÄŸinde

### Edward Hu'nun DehasÄ±

2021'de Microsoft Research'Ã¼n koridorlarÄ±nda Edward Hu, yapay zeka topluluÄŸunu yÄ±llarca uÄŸraÅŸtÄ±ran matematiksel bir bulmacayla boÄŸuÅŸuyordu. Devasa bir sinir aÄŸÄ±na tam olarak yeniden eÄŸitmeden nasÄ±l yeni numaralar Ã¶ÄŸretirsiniz?

Geleneksel bilgelik, her aÄŸÄ±rlÄ±ÄŸÄ± gÃ¼ncellemeniz gerektiÄŸini sÃ¶ylÃ¼yorduâ€”GPT-3'Ã¼n durumunda 175 milyarÄ±n tamamÄ±nÄ±. Bu, sadece yeni bir resim asmak iÃ§in tÃ¼m evinizi yeniden inÅŸa etmeniz gerektiÄŸinde Ä±srar etmek gibiydi.

Ama Hu'nun farklÄ± bir fikri vardÄ±. Ya yapmanÄ±z gereken "deÄŸiÅŸiklikler" aslÄ±nda o kadar karmaÅŸÄ±k deÄŸilse? Ya bilginin Ã§oÄŸu zaten oradaysa ve sadece birkaÃ§ stratejik "adaptÃ¶r" eklemeniz gerekiyorsa?

**AtÄ±lÄ±m AnÄ±**

Hu derin bir ÅŸeyi fark etti: fine-tuning iÃ§in gereken gÃ¼ncellemelerin genellikle dÃ¼ÅŸÃ¼k iÃ§sel boyutluluklarÄ± vardÄ±. Basit terimlerle, yapmanÄ±z gereken deÄŸiÅŸiklikler Ã§ok daha kÃ¼Ã§Ã¼k matematiksel yapÄ±lar kullanÄ±larak ifade edilebilir.

Devasa bir W matrisini doÄŸrudan gÃ¼ncellemek yerine:
```python
W_new = W + Î”W  # Î”W devasa ve pahalÄ±
```

GÃ¼ncellemeyi iki minik matrise ayÄ±rdÄ±:
```python
W_new = W + B Ã— A  # B ve A Ã§ok, Ã§ok daha kÃ¼Ã§Ã¼k
```

Bu, tÃ¼m ansiklopediyi yeniden yazmak yerine, sadece doÄŸru deÄŸiÅŸiklikleri gÃ¶steren kÃ¼Ã§Ã¼k bir dizin ekleyebileceÄŸinizi keÅŸfetmek gibiydi.

**Sihirli SayÄ±lar**

Hu, LoRA'yÄ± (Low-Rank Adaptation) yayÄ±nladÄ±ÄŸÄ±nda, sonuÃ§lar ÅŸaÅŸÄ±rtÄ±cÄ±ydÄ±:
- **HafÄ±za kullanÄ±mÄ± %95 dÃ¼ÅŸtÃ¼** - bazÄ± modeller iÃ§in 32GB'den 1.5GB'ye
- **EÄŸitim sÃ¼resi %90 azaldÄ±** - gÃ¼nlerden saatlere
- **Performans aynÄ± kaldÄ±** - hiÃ§bir kalite kaybÄ± olmadÄ±

Aniden, masanÄ±zda duran GMKtec K11 artÄ±k bir oyuncak deÄŸildi. MeÅŸru bir yapay zeka araÅŸtÄ±rma iÅŸ istasyonuydu.

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: Bu tam olarak `02-huggingface-peft/` modÃ¼lÃ¼mÃ¼zÃ¼n LoRA ile baÅŸlamasÄ±nÄ±n nedeni - diÄŸer her ÅŸeyi mÃ¼mkÃ¼n kÄ±lan temeldir. K11'de ilk LoRA fine-tune'unuzu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda, Hu'nun adÄ±mlarÄ±nÄ± takip ediyorsunuz.

**ğŸ“š Daha Fazla Bilgi:**
- **Orijinal Makale**: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- **HuggingFace PEFT**: [https://github.com/huggingface/peft](https://github.com/huggingface/peft)
- **Microsoft Research Blog**: [LoRA: Adapting Large Language Models](https://www.microsoft.com/en-us/research/blog/lora-adapting-large-language-models/)

---

## BÃ¶lÃ¼m 2: NVIDIA Devrimi - Ä°yi OlanÄ±n Harika OlmasÄ±

### LoRA'yÄ± RahatsÄ±z Eden Gizem

ÃœÃ§ yÄ±l boyunca, LoRA verimli fine-tuning'in tartÄ±ÅŸmasÄ±z kralÄ±ydÄ±. DÃ¼nyanÄ±n dÃ¶rt bir yanÄ±ndaki araÅŸtÄ±rmacÄ±lar onu kullandÄ±, sevdi ve kariyerlerini onun Ã¼zerine kurdu. Ama NVIDIA'daki Shih-Yang Liu rahatsÄ±z edici bir duyguyu atamÄ±yordu.

LoRA inanÄ±lmaz derecede iyi Ã§alÄ±ÅŸÄ±yordu, ama *neden*? Ve daha da Ã¶nemlisi, neyi kaÃ§Ä±rÄ±yordu?

Liu, sinir aÄŸÄ± aÄŸÄ±rlÄ±klarÄ±nÄ±n matematiÄŸine derinlemesine dalmak iÃ§in aylar harcadÄ±. Sadece sayÄ±lara bakmÄ±yorduâ€”onlarÄ±n *Ã¶zÃ¼nÃ¼* anlamaya Ã§alÄ±ÅŸÄ±yordu. Bir aÄŸÄ±rlÄ±k matrisini neyin iÅŸlettiÄŸini anlamaya Ã§alÄ±ÅŸÄ±yordu.

**AydÄ±nlanma AnÄ±**

2024'te bir akÅŸam, Liu'nun atÄ±lÄ±mÄ± gerÃ§ekleÅŸti. Her aÄŸÄ±rlÄ±k matrisinin iki temel bileÅŸene sahip olarak dÃ¼ÅŸÃ¼nÃ¼lebileceÄŸini fark etti:
- **BÃ¼yÃ¼klÃ¼k**: BaÄŸlantÄ±nÄ±n ne kadar "gÃ¼Ã§lÃ¼" olduÄŸu
- **YÃ¶n**: BaÄŸlantÄ±nÄ±n hangi yÃ¶ne iÅŸaret ettiÄŸi

Bu, fizikte bir vektÃ¶rÃ¼ tanÄ±mlamak gibiydiâ€”onu tam olarak anlamak iÃ§in hem gÃ¼Ã§ hem de yÃ¶ne ihtiyacÄ±nÄ±z vardÄ±r.

Sonra ÅŸok edici fark geldi: **LoRA sadece yÃ¶nÃ¼ uyarlÄ±yordu!**

```python
# LoRA'nÄ±n aslÄ±nda ne yaptÄ±ÄŸÄ± (fark etmeden)
W = bÃ¼yÃ¼klÃ¼k Ã— yÃ¶n
LoRA_update = sadece_yÃ¶n_deÄŸiÅŸimi  # Resmin yarÄ±sÄ± eksik!

# DoRA'nÄ±n Ã¶nerdiÄŸi
W = bÃ¼yÃ¼klÃ¼k Ã— yÃ¶n  
DoRA_update = bÃ¼yÃ¼klÃ¼k_deÄŸiÅŸimi + yÃ¶n_deÄŸiÅŸimi  # Tam resim!
```

**Her Åeyi DeÄŸiÅŸtiren Deney**

Liu ve ekibi DoRA'yÄ± (Weight-Decomposed Low-Rank Adaptation) test ettiÄŸinde, sonuÃ§lar nefes kesiciydi:

- **Llama 7B akÄ±l yÃ¼rÃ¼tme gÃ¶revlerinde 3.7 puan iyileÅŸti**
- **Llama 3 8B performansta 4.4 puan sÄ±Ã§radÄ±**
- **Test edilen her tek model** tutarlÄ± iyileÅŸmeler gÃ¶sterdi
- **Ek hafÄ±za maliyeti yok** - LoRA ile aynÄ± verimlilik

Bu, sanatÃ§Ä±larÄ±n ÅŸimdiye kadar paletlerinin sadece yarÄ±sÄ±yla resim yaptÄ±klarÄ±nÄ± keÅŸfetmek gibiydi.

**GerÃ§ek DÃ¼nya Etkisi**

SayÄ±lar hikayeyi anlatÄ±yor:
- **SaÄŸduyu akÄ±l yÃ¼rÃ¼tme**: +3.7 puan (bu AI'da Ã§ok bÃ¼yÃ¼k!)
- **Ã‡ok turlu konuÅŸmalar**: +0.4 puan (daha doÄŸal diyalog)
- **GÃ¶rme gÃ¶revleri**: +1.9 puan (daha iyi gÃ¶rÃ¼ntÃ¼ anlama)

BaÄŸlam iÃ§in, AI kÄ±yaslamalarÄ±nda 1 puanlÄ±k iyileÅŸme Ã¶nemli kabul edilir. DoRA tutarlÄ± bir ÅŸekilde 3-4 puanlÄ±k sÄ±Ã§ramalar gerÃ§ekleÅŸtiriyordu.

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: DoRA, `10-cutting-edge-peft/` modÃ¼lÃ¼mÃ¼zÃ¼n yÄ±ldÄ±zÄ±dÄ±r. K11'de LoRA ve DoRA performansÄ±nÄ± karÅŸÄ±laÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda, bu tam iyileÅŸtirmeleri iÅŸ baÅŸÄ±nda gÃ¶receksiniz. Bu, iyi ve harika fine-tuning arasÄ±ndaki farktÄ±r.

**ğŸ“š Daha Fazla Bilgi:**
- **Orijinal Makale**: [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353)
- **NVIDIA GeliÅŸtirici Blogu**: [Introducing DoRA, a High-Performing Alternative to LoRA](https://developer.nvidia.com/blog/introducing-dora-a-high-performing-alternative-to-lora-for-fine-tuning/)
- **DoRA Uygulama**: [NVlabs/DoRA](https://github.com/NVlabs/DoRA)

---

## BÃ¶lÃ¼m 3: TopluluÄŸun CevabÄ± - Parlak Zihinler Ä°ÅŸ BirliÄŸi YaptÄ±ÄŸÄ±nda

### MÃ¼kemmelliÄŸin Problemi

DoRA muhteÅŸemdi, ama bir yakalama noktasÄ± vardÄ±: hala LoRA ile aynÄ± hafÄ±zayÄ± gerektiriyordu. 2GB VRAM'li sevgili GMKtec K11'imiz gibi mÃ¼tevazÄ± donanÄ±mlÄ± birÃ§ok araÅŸtÄ±rmacÄ± iÃ§in, daha bÃ¼yÃ¼k modellerle Ã§alÄ±ÅŸÄ±rken LoRA bile zorlanabiliyordu.

Yapay zekanÄ±n milyonlarca dolarlÄ±k GPU kÃ¼melerine sahip olanlara deÄŸil, herkese eriÅŸilebilir olmasÄ± gerektiÄŸine inanan yenilikÃ§i araÅŸtÄ±rma laboratuvarÄ± Answer.AI devreye girdi.

### Ä°ki Dahi Fikrin EvliliÄŸi

Answer.AI'daki ekip DoRA'ya baktÄ± ve Ã§Ä±lgÄ±n bir fikir edindi: "Ya bunu quantization ile birleÅŸtirirsek?"

Quantization, aÅŸina olmayanlar iÃ§in, Ã¶nemli detaylarÄ± kaybetmeden yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ fotoÄŸrafÄ± sÄ±kÄ±ÅŸtÄ±rmak gibidir. SayÄ±larÄ± 16 bit hassasiyetle depolamak yerine, genellikle sadece 4 bit ile yetinebilirsinizâ€”4 kat hafÄ±za azalmasÄ±!

Sorun, quantization'Ä±n her zaman LoRA ile eÅŸleÅŸtirilmiÅŸ olmasÄ±ydÄ±. Kimse onu Ã¼stÃ¼n DoRA ile karÄ±ÅŸtÄ±rmayÄ± denememiÅŸti.

**Topluluk Deneyi**

Sonra olan ÅŸey gÃ¼zeldi. Answer.AI, QDoRA'yÄ± (Quantized DoRA) izolasyonda geliÅŸtirmedi. Toplulukla iÅŸ birliÄŸi yaptÄ±lar, deneyleri paylaÅŸtÄ±lar, geri bildirim topladÄ±lar ve hÄ±zla iterasyon yaptÄ±lar.

SonuÃ§? **QDoRA**â€”size ÅŸunlarÄ± veren bir teknik:
- **DoRA'nÄ±n Ã¼stÃ¼n performansÄ±** (LoRA'ya gÃ¶re +3.7 puan)
- **Quantization'Ä±n hafÄ±za verimliliÄŸi** (4 kat hafÄ±za azalmasÄ±)
- **Her iki dÃ¼nyanÄ±n en iyisi** tek bir teknikte

```python
# Sihirli kombinasyon
QDoRA = DoRA_performansÄ± + Quantization_verimliliÄŸi
# SonuÃ§: 1/4 hafÄ±zada Ã¼stÃ¼n adaptasyon
```

**K11 BaÄŸlantÄ±sÄ±**

Bu, GMKtec K11 gibi donanÄ±mlar iÃ§in oyun deÄŸiÅŸtirici oldu. Aniden, ÅŸunlarÄ± yapabiliyordunuz:
- **7B modelleri 2GB VRAM'de Ã§alÄ±ÅŸtÄ±rma** (daha Ã¶nce imkansÄ±zdÄ±)
- **Quantization verimliliÄŸi ile DoRA seviyesi performans** alma
- Tam fine-tuning'i geÃ§en modeller eÄŸitme
- TÃ¼m bunlarÄ± tÃ¼ketici masaÃ¼stÃ¼nde yapma

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: QDoRA, `04-quantization/` ve `10-cutting-edge-peft/` modÃ¼llerimizde Ã¶ne Ã§Ä±karÄ±lmÄ±ÅŸtÄ±r. CLAUDE.md'de "EXTREME_CONFIG" gÃ¶rdÃ¼ÄŸÃ¼nÃ¼zde (1 batch boyutu, 4-bit quantization), QDoRA'yÄ± K11'de mÃ¼mkÃ¼n kÄ±lan tam hafÄ±za-verimli teknikleri kullanÄ±yorsunuz.

**ğŸ“š Daha Fazla Bilgi:**
- **Answer.AI Blogu**: [QDoRA: Quantized DoRA Fine-tuning](https://www.answer.ai/posts/2024-03-14-qdora.html)
- **Topluluk TartÄ±ÅŸmasÄ±**: [QDoRA Implementation Thread](https://github.com/huggingface/peft/discussions/1474)
- **Quantization Rehberi**: [BitsAndBytes Documentation](https://github.com/TimDettmers/bitsandbytes)

---

## BÃ¶lÃ¼m 4: MatematikÃ§inin Ä°Ã§gÃ¶rÃ¼sÃ¼ - Neden Rastgele YanlÄ±ÅŸtÄ±r

### Her Åeyi BaÅŸlatan RahatsÄ±z Edici Soru

DoRA manÅŸetlerde yer alÄ±rken, farklÄ± bir araÅŸtÄ±rmacÄ± grubu rahatsÄ±z edici bir soru soruyordu: "Neden LoRA adaptÃ¶rlerini rastgele sayÄ±larla baÅŸlatÄ±yoruz?"

Bunu dÃ¼ÅŸÃ¼nÃ¼n. Ä°nsan bilgisini kodlamak iÃ§in devasa modelleri eÄŸitmek iÃ§in muazzam Ã§aba harcÄ±yoruz. Bu modeller milyarlarca metin Ã¶rneÄŸinden Ã¶ÄŸrenilen kalÄ±plarÄ± iÃ§erir. AÄŸÄ±rlÄ±k matrisleri dil, akÄ±l yÃ¼rÃ¼tme ve bilginin sÄ±rlarÄ±nÄ± barÄ±ndÄ±rÄ±r.

Ve sonra, onlarÄ± uyarlamak istediÄŸimizde, tamamen rastgele gÃ¼rÃ¼ltÃ¼ ile baÅŸlÄ±yoruz?

Bu, usta ÅŸefin mÃ¼kemmel ÅŸekilde hazÄ±rladÄ±ÄŸÄ± tarife sahip olmak, sonra da market dÃ¼kkanÄ±na dart atarak seÃ§ilen malzemeler eklemek gibiydi.

### Temel BileÅŸen Devrimi

PiSSA'nÄ±n (Principal Singular Values and Singular Vectors Adaptation) arkasÄ±ndaki matematikÃ§iler daha iyi bir fikri vardÄ±. Rastgele baÅŸlatma yerine, modelin zaten bildiÄŸi **en Ã¶nemli parÃ§alar** ile baÅŸlasak?

**Matematiksel Sihir**

Her aÄŸÄ±rlÄ±k matrisi, Singular Value Decomposition (SVD) adÄ± verilen bir ÅŸey kullanÄ±larak ayrÄ±ÅŸtÄ±rÄ±labilir. Bunu karmaÅŸÄ±k bir senfoniyi en Ã¶nemli mÃ¼zik temalarÄ±na ayÄ±rmak olarak dÃ¼ÅŸÃ¼nÃ¼n:

```python
# Geleneksel LoRA: Rastgele gÃ¼rÃ¼ltÃ¼ ile baÅŸla
A = rastgele_gÃ¼rÃ¼ltÃ¼()  # Herhangi bir yere iÅŸaret edebilir!
B = sÄ±fÄ±rlar()         # BaÅŸlangÄ±Ã§ta hiÃ§bir katkÄ± yapmaz

# PiSSA: Modelin en Ã¶nemli kalÄ±plarÄ± ile baÅŸla
U, S, V = SVD(orijinal_aÄŸÄ±rlÄ±k)  # "Senfoni"yi ayrÄ±ÅŸtÄ±r
A = U_en_Ã¶nemli            # Anahtar "temalar" ile baÅŸla  
B = S_en_Ã¶nemli @ V_en_Ã¶nemli  # Ã–nem aÄŸÄ±rlÄ±klarÄ±
```

**Parlak Ä°Ã§gÃ¶rÃ¼**

PiSSA araÅŸtÄ±rmacÄ±larÄ±, Ã¼st tekil deÄŸerlerin ve vektÃ¶rlerin modelin Ã¶ÄŸrendiÄŸi en kritik kalÄ±plarÄ± temsil ettiÄŸini fark ettiler. AdaptÃ¶rleri bu bileÅŸenlerle baÅŸlatarak, fine-tuning sÃ¼recine temelde ÅŸunu sÃ¶ylÃ¼yorsunuz: "Buradan baÅŸlaâ€”Ã¶nemli olan bu."

Bu, Ã¶ÄŸrenciye yeni materyal Ã¶ÄŸrenmesini istemeden Ã¶nce ders kitabÄ±nÄ±n en Ã¶nemli bÃ¶lÃ¼mlerini vermek gibiydi.

### Herkesi Åok Eden SonuÃ§lar

PiSSA test edildiÄŸinde, iyileÅŸmeler anÄ±nda ve tutarlÄ±ydÄ±:

- **Daha hÄ±zlÄ± yakÄ±nsama**: Modeller yeni gÃ¶revleri daha az adÄ±mda Ã¶ÄŸrendi
- **Daha iyi kararlÄ±lÄ±k**: EÄŸitim daha dÃ¼zgÃ¼n ve Ã¶ngÃ¶rÃ¼lebilirdi
- **ÃœstÃ¼n final performansÄ±**: Son sonuÃ§lar tutarlÄ± bir ÅŸekilde rastgele baÅŸlatmayÄ± geÃ§ti
- **Prensipli yaklaÅŸÄ±m**: Sonunda, adaptasyonu baÅŸlatmanÄ±n matematiksel olarak saÄŸlam yolu

**Ã–ÄŸrenme HÄ±zÄ± Devrimi**

Belki de en Ã¶nemlisi, PiSSA modelleri daha hÄ±zlÄ± Ã¶ÄŸrendi. K11'deki eÄŸitim ortamÄ±mÄ±zda bu ÅŸu anlama geliyor:
- **Daha kÄ±sa eÄŸitim sÃ¼releri** (45-60 dakika yerine 15-30 dakika)
- **Daha az elektrik maliyeti** (uzun eÄŸitim oturumlarÄ± iÃ§in Ã¶nemli)
- **Daha hÄ±zlÄ± deney dÃ¶ngÃ¼leri** (aynÄ± sÃ¼rede daha fazla fikir deneyin)

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: PiSSA teknikleri, `10-cutting-edge-peft/` geliÅŸmiÅŸ yÃ¶ntemlerimize entegre edilmiÅŸtir. EÄŸitimlerde baÅŸlatma stratejilerini karÅŸÄ±laÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda, "akÄ±llÄ±" baÅŸlamanÄ±n "rastgele" baÅŸlamayÄ± her zaman nasÄ±l yendiÄŸini gÃ¶receksiniz. Bu Ã¶zellikle `00-first-time-beginner/` Qwen2.5 0.6B Ã¶rneklerimizde fark edilirâ€”rastgele baÅŸlatma ile 30 dakika alan aynÄ± model, PiSSA ile 15 dakikada yakÄ±nsayabilir!

**ğŸ“š Daha Fazla Bilgi:**
- **AraÅŸtÄ±rma Makalesi**: [PiSSA: Principal Singular Values and Singular Vectors Adaptation](https://arxiv.org/abs/2404.02948)
- **Uygulama**: [GraphPKU/PiSSA](https://github.com/GraphPKU/PiSSA)
- **Matematiksel Arka Plan**: [Singular Value Decomposition Explained](https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm)

---

## BÃ¶lÃ¼m 5: Hizalama Devrimi - Yapay Zeka Kendi Ã–ÄŸretmeni OlduÄŸunda

### Ä°nsan DarboÄŸazÄ± Problemi

2024'te, yapay zeka geliÅŸtirmede yeni bir kriz ortaya Ã§Ä±kÄ±yordu. Modeller inanÄ±lmaz derecede gÃ¼Ã§lÃ¼ hale geliyordu, ama onlarÄ± yardÄ±msever, zararsÄ±z ve dÃ¼rÃ¼st yapmak, Ä°nsan Geri Bildiriminden GÃ¼Ã§lendirmeli Ã–ÄŸrenme (RLHF) adÄ± verilen bir ÅŸey gerektiriyordu.

SÃ¼reÃ§ basit ama pahalÄ±ydÄ±: insanlara binlerce model yanÄ±tÄ± gÃ¶sterin, hangilerinin daha iyi olduÄŸunu deÄŸerlendirmelerini isteyin, sonra o geri bildirimi modelin iyi davranmayÄ± Ã¶ÄŸrenmesi iÃ§in kullanÄ±n.

Sadece bir problem vardÄ±: **insanlar pahalÄ±, yavaÅŸ ve tutarsÄ±zdÄ±lar.**

Tek bir hizalama Ã§alÄ±ÅŸmasÄ± insan aÃ§Ä±klama Ã¼cretlerinde 50.000 dolara mal olabilirdi. Daha da kÃ¶tÃ¼sÃ¼, insanlar yorulurlar, birbirleriyle anlaÅŸmazlÄ±ÄŸa dÃ¼ÅŸerler ve 7/24 Ã§alÄ±ÅŸamazlar. Bu, her Ã¶ÄŸrencinin kiÅŸisel Ã¶ÄŸretmene sahip olmasÄ±nÄ± gerektirerek eÄŸitimi Ã¶lÃ§eklendirmeye Ã§alÄ±ÅŸmak gibiydiâ€”teoride asil, pratikte imkansÄ±z.

### Anayasal Yapay Zeka AtÄ±lÄ±mÄ±

Anthropic'teki (Claude'u yaratan ÅŸirket) Lee ve meslektaÅŸlarÄ± devrimci bir fikri vardÄ±: Ya yapay zekaya kendini Ã¶ÄŸretmeyi Ã¶ÄŸretebilirsek?

Ä°nsan geri bildirimi yerine gÃ¼Ã§lÃ¼ bir yapay zeka modelinin (GPT-4 gibi) yazÄ±lÄ± bir ilkeler "anayasasÄ±"na gÃ¶re yanÄ±tlarÄ± deÄŸerlendirip geliÅŸtirdiÄŸi Anayasal Yapay Zeka adlÄ± bir ÅŸey geliÅŸtirdiler.

**RLAIF'in Sihri**

RLAIF (Reinforcement Learning from AI Feedback) senaryoyu tamamen deÄŸiÅŸtirdi:

```python
# Geleneksel RLHF: PahalÄ± ve yavaÅŸ
insan_deÄŸerlendirmesi = pahalÄ±_insan_aÃ§Ä±klayÄ±cÄ±sÄ±(model_yanÄ±tÄ±)
model.Ã¶ÄŸren(insan_deÄŸerlendirmesi)

# RLAIF: HÄ±zlÄ± ve Ã¶lÃ§eklenebilir
ai_deÄŸerlendirmesi = gpt4_anayasal_deÄŸerlendiricisi(model_yanÄ±tÄ±, anayasa)
model.Ã¶ÄŸren(ai_deÄŸerlendirmesi)
```

**Åok Edici SonuÃ§lar**

Diyalog ve Ã¶zetleme gÃ¶revlerinde test edildiÄŸinde, RLAIF modelleri RLHF modelleri ile **aynÄ±** performansÄ± gÃ¶sterdi. Yapay zeka geri bildirimi insan geri bildirimi kadar iyiydi, ama:

- **1000 kat daha ucuz**: 50.000 dolar yerine 500 dolar
- **1000 kat daha hÄ±zlÄ±**: haftalĞ°Ñ€ yerine dakikalar
- **Sonsuz Ã¶lÃ§eklenebilir**: insan yorgunluÄŸu veya eriÅŸilebilirlik kÄ±sÄ±tlamasÄ± yok
- **Daha tutarlÄ±**: yapay zekanÄ±n kÃ¶tÃ¼ gÃ¼nleri veya anlaÅŸmazlÄ±klarÄ± yok

### Anayasal EÄŸitim Devrimi

AtÄ±lÄ±m sadece maliyet tasarrufundan daha derine gitti. Anayasal Yapay Zeka, araÅŸtÄ±rmacÄ±larÄ±n karmaÅŸÄ±k etik ilkeleri doÄŸrudan eÄŸitime kodlamasÄ±na izin verdi:

- **YardÄ±mseverlik**: DoÄŸru kalÄ±rken maksimum derecede yardÄ±mcÄ± olun
- **ZararsÄ±zlÄ±k**: ZararlÄ± iÃ§erik Ã¼retmekten kaÃ§Ä±nÄ±n
- **DÃ¼rÃ¼stlÃ¼k**: HalÃ¼sinasyon yapmak yerine belirsizliÄŸi kabul edin

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: Bu, `07-system-prompt-modification/` ve `12-advanced-rlhf/` modÃ¼llerimizin temelidir. K11'de sansÃ¼rsÃ¼z modeller oluÅŸturmak iÃ§in anayasal eÄŸitim kullandÄ±ÄŸÄ±nÄ±zda, bÃ¼yÃ¼k yapay zeka laboratuvarlarÄ±nÄ±n kullandÄ±ÄŸÄ± aynÄ± teknikleri kullanÄ±yorsunuzâ€”ama kendi Ã¶zel ihtiyaÃ§larÄ±nÄ±za uyarlanmÄ±ÅŸ. CLAUDE.md'de bahsedilen "20-40 dakikalÄ±k eÄŸitim sÃ¼resi"? Bu, RLAIF'in geliÅŸmiÅŸ hizalamayÄ± tÃ¼ketici donanÄ±mÄ±nda eriÅŸilebilir kÄ±lmasÄ±dÄ±r.

**DemokratikleÅŸme Etkisi**

Belki de en Ã¶nemlisi, RLAIF geliÅŸmiÅŸ yapay zeka hizalamasÄ±nÄ± demokratikleÅŸtirdi. Daha Ã¶nce sadece milyonlarca dolarlÄ±k bÃ¼tÃ§eli laboratuvarlar RLHF'yi karÅŸÄ±layabiliyordu. Åimdi, iyi bir bilgisayarÄ± olan herkes (GMKtec K11 gibi) hizalanmÄ±ÅŸ, yardÄ±msever yapay zeka sistemleri eÄŸitebiliyordu.

Bu, tam orkestra kiralamak zorunda kalmak ile dijital ses iÅŸ istasyonu ile senfoniler yaratabilmek arasÄ±ndaki fark gibiydi.

**ğŸ“š Daha Fazla Bilgi:**
- **Anayasal Yapay Zeka Makalesi**: [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)
- **RLAIF AraÅŸtÄ±rmasÄ±**: [RLAIF: Scaling Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2309.00267)
- **Anthropic Blogu**: [Constitutional AI: Harmlessness from AI Feedback](https://www.anthropic.com/news/constitutional-ai-harmlessness-from-ai-feedback)
- **Uygulama Rehberi**: [TRL Constitutional Training](https://huggingface.co/docs/trl/constitutional_ai)

## BÃ¶lÃ¼m 6: LLaMA-Factory Devrimi - KarmaÅŸÄ±klÄ±ÄŸÄ±n Basit Hale Gelmesi

### Ã‡ok Fazla SeÃ§enek Problemi

2024'te, fine-tuning manzarasÄ± inanÄ±lmaz derecede zenginâ€”ve inanÄ±lmaz derecede kafa karÄ±ÅŸtÄ±rÄ±cÄ± hale gelmiÅŸti. LoRA, DoRA, QLoRA, QDoRA, PiSSA, RLHF, RLAIF ve dÃ¼zinelerce baÅŸka tekniÄŸiniz vardÄ±. Her biri gÃ¼Ã§lÃ¼ydÃ¼, ama hepsini Ã¶ÄŸrenmek dÃ¼nyadaki her mutfaÄŸÄ± piÅŸirmeyi Ã¶ÄŸrenerek usta aÅŸÃ§Ä± olmaya Ã§alÄ±ÅŸmak gibiydi.

"BÃ¼tÃ¼n bu tekniklere tek, gÃ¼zel bir arayÃ¼z aracÄ±lÄ±ÄŸÄ±yla eriÅŸebilsek ne olur?" diye basit bir soru soran **LLaMA-Factory** projesini girin.

### Kod Yazmadan Devrim

LLaMA-Factory'nin yaratÄ±cÄ±larÄ± derin bir ÅŸeyi fark ettiler: modelleri fine-tune etmeye ihtiyacÄ± olan herkes programcÄ± deÄŸildir. Doktorlar, avukatlar, araÅŸtÄ±rmacÄ±lar, yazarlar ve giriÅŸimcilerin tÃ¼mÃ¼ yapay zekayÄ± geliÅŸtirebilecek alan uzmanlÄ±ÄŸÄ±na sahiptirâ€”ama katkÄ±da bulunmak iÃ§in Python, CUDA ve daÄŸÄ±tÄ±k eÄŸitim Ã¶ÄŸrenmeleri gerekmemelidir.

**Web ArayÃ¼zÃ¼ AtÄ±lÄ±mÄ±**

LLaMA-Factory devrimci bir ÅŸey tanÄ±ttÄ±: fine-tuning iÃ§in bir web arayÃ¼zÃ¼. ÅunlarÄ± yapabiliyordunuz:

- **100+ model arasÄ±ndan seÃ§im** en son sÃ¼rÃ¼mler dahil
- **EÄŸitim yÃ¶nteminizi seÃ§in** (LoRA, DoRA, RLHF) aÃ§Ä±lÄ±r menÃ¼lerden
- **Verilerinizi yÃ¼kleyin** dosyalarÄ± sÃ¼rÃ¼kleyip bÄ±rakarak
- **EÄŸitimi izleyin** gerÃ§ek zamanlÄ± grafikler ve Ã§izelgelerle
- **Modelleri dÄ±ÅŸa aktarÄ±n** ihtiyacÄ±nÄ±z olan herhangi bir formata

```bash
# Her ÅŸeyi deÄŸiÅŸtiren sihirli komut
llamafactory-cli webui
# TarayÄ±cÄ±nÄ±zda gÃ¼zel bir arayÃ¼z aÃ§ar
```

**GÃ¼n-0 Vaadi**

Belki de en dikkat Ã§ekici olanÄ±, LLaMA-Factory'nin yeni modeller iÃ§in "GÃ¼n-0" desteÄŸi taahhÃ¼dÃ¼ vermiÅŸ olmasÄ±ydÄ±. Meta Llama 3.1'i yayÄ±nladÄ±ÄŸÄ±nda, saatler iÃ§inde LLaMA-Factory'de destekleniyordu. Qwen2.5 Ã§Ä±ktÄ±ÄŸÄ±nda, hemen oradaydÄ±.

Bu sadece kolaylÄ±k deÄŸildiâ€”devrimdi. Daha Ã¶nce, framework desteÄŸi iÃ§in 3-6 ay beklemek normaldi. LLaMA-Factory, son teknoloji modelleri anÄ±nda eriÅŸilebilir kÄ±ldÄ±.

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: Bu yÃ¼zden Ã¶ÄŸrenme yolculuÄŸumuza `08-llamafactory/` ekledik. 01-07 modÃ¼llerindeki temelleri Ã¶ÄŸrendikten sonra, LLaMA-Factory geliÅŸmiÅŸ deneyim iÃ§in "gÃ¶rev kontrolÃ¼nÃ¼z" haline gelir. Web arayÃ¼zÃ¼, yapÄ±landÄ±rma dosyalarÄ±na deÄŸil veri ve sonuÃ§lara odaklanmak isteyen Ã¶ÄŸrenciler iÃ§in mÃ¼kemmeldir.

### Ãœretim Pipeline RÃ¼yasÄ±

Ama LLaMA-Factory daha da ileri gitti. Sadece fine-tuning'i kolay hale getirmekle ilgili deÄŸildiâ€”**tam** hale getirmekle ilgiliydi. Yeni baÅŸlayanlara yardÄ±m eden aynÄ± araÃ§ aynÄ± zamanda ÅŸunlarÄ± da destekliyordu:

- **DeepSpeed entegrasyonu** ile Ã§oklu-GPU eÄŸitimi
- **Wandb ve TensorBoard** ile deney takibi  
- **KapsamlÄ± kÄ±yaslamalar** ile model deÄŸerlendirmesi
- **vLLM ve Ollama dÄ±ÅŸa aktarÄ±mÄ±** ile daÄŸÄ±tÄ±m pipeline'larÄ±

Bu, gerektiÄŸinde diferansiyel denklemleri de Ã§Ã¶zebilen basit bir hesap makinesi olmasÄ± gibiydi.

**ğŸ“š Daha Fazla Bilgi:**
- **LLaMA-Factory GitHub**: [hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- **DokÃ¼mantasyon**: [LLaMA-Factory Docs](https://llamafactory.readthedocs.io/)
- **Web ArayÃ¼zÃ¼ Demosu**: [LLaMA-Factory WebUI Tutorial](https://github.com/hiyouga/LLaMA-Factory/wiki/Web-UI)
- **Model DesteÄŸi**: [Supported Models List](https://github.com/hiyouga/LLaMA-Factory#supported-models)

## Son: Gelecek Sizin Ellerinizde

### BugÃ¼n Nerede Duruyoruz

2024'Ã¼n sonuna geldikÃ§e ve 2025'e bakarken, fine-tuning devrimi yapay zeka ile mÃ¼mkÃ¼n olanÄ± temelden deÄŸiÅŸtirdi. KeÅŸfettiÄŸimiz tekniklerâ€”Hu'nun temel LoRA'sÄ±ndan Liu'nun Ã§Ä±ÄŸÄ±r aÃ§an DoRA'sÄ±naâ€”sadece akademik baÅŸarÄ±lardan daha fazlasÄ±nÄ± temsil ediyor. Yapay zekanÄ±n demokratikleÅŸtirilmesini temsil ediyorlar.

**KiÅŸisel Bilgisayar AnÄ±**

Yapay zekanÄ±n kiÅŸisel bilgisayar devriminin eÅŸdeÄŸerini yaÅŸÄ±yoruz. KiÅŸisel bilgisayar, bilgisayarlarÄ± kurumsal ana bilgisayarlardan bireysel masalara taÅŸÄ±dÄ±ÄŸÄ± gibi, bu fine-tuning atÄ±lÄ±mlarÄ± yapay zeka geliÅŸtirmeyi milyar dolarlÄ±k laboratuvarlardan kiÅŸisel iÅŸ istasyonlarÄ±na taÅŸÄ±yor.

MÃ¼tevazÄ± 2GB VRAM'li GMKtec K11'iniz ÅŸimdi ÅŸunlarÄ± yapabilir:
- GPT-3'e rakip yeteneklerde modelleri fine-tune etmek
- Genel modellerden daha iyi performans gÃ¶steren alan-spesifik yapay zeka eÄŸitmek
- Sadece aylar Ã¶nce var olmayan tekniklerle deney yapmak
- Devasa bÃ¼tÃ§eler olmadan hizalanmÄ±ÅŸ, yardÄ±msever yapay zeka oluÅŸturmak

### Ã–nÃ¼mÃ¼zdeki Ã–ÄŸrenme YolculuÄŸu

**EÄŸitmenler Ä°Ã§in: Devrimi Ã–ÄŸretmek**

Bu kavramlarÄ± Ã¶ÄŸrettiÄŸinizde, sadece teknikleri aÃ§Ä±klamadÄ±ÄŸÄ±nÄ±zÄ± hatÄ±rlayÄ±nâ€”geleceÄŸin anahtarlarÄ±nÄ± paylaÅŸÄ±yorsunuz. Bu becerilerde ustalaÅŸan her Ã¶ÄŸrenci, daha Ã¶nce doktora araÅŸtÄ±rmacÄ±larÄ± ekipleri gerektiren problemleri Ã§Ã¶zebilir hale gelir.

**Dersleriniz Ä°Ã§in ÃœÃ§ Perdeli YapÄ±:**

**Perde I: Problem** (Ä°mkansÄ±zlÄ±k ile kancalayÄ±n)
- "ChatGPT'yi Ã¶zelleÅŸtirmek istediÄŸinizi hayal edin, ama eÄŸitim 12 milyon dolara mal oluyor..."
- Ä°lerlemeyi engelleyen duvarÄ± gÃ¶sterin

**Perde II: AtÄ±lÄ±m** (DehayÄ± ortaya Ã§Ä±karÄ±n)
- Her keÅŸfi anlatÄ±n: LoRA, DoRA, PiSSA, RLAIF
- Analojiler kullanÄ±n: heykeltÄ±raÅŸlar, senfoniler, usta aÅŸÃ§Ä±lar
- Somut sayÄ±lar gÃ¶sterin: +3.7 puan, %95 hafÄ±za tasarrufu

**Perde III: Gelecek** (KatkÄ±da bulunmalarÄ± iÃ§in ilham verin)
- "SÄ°Z hangi atÄ±lÄ±mÄ± keÅŸfedeceksiniz?"
- "Bu hikayenin sonraki bÃ¶lÃ¼mÃ¼ yazÄ±lmamÄ±ÅŸ"
- "AraÃ§lar artÄ±k sizin elinizde"

### **EtkileÅŸimli Ã–ÄŸretim Teknikleri**

**HeykeltÄ±raÅŸ Egzersizi**
Ã–ÄŸrencilere gerÃ§ek kil verin. BazÄ±larÄ± kaba araÃ§lar (LoRA), diÄŸerleri hassas enstrÃ¼manlar (DoRA) kullansÄ±n. Hangi heykeller daha iyi Ã§Ä±kar? Neden?

**HafÄ±za Oyunu**
FarklÄ± model konfigÃ¼rasyonlarÄ±nÄ± yÃ¼klerken RAM kullanÄ±mÄ±nÄ± gerÃ§ek zamanlÄ± gÃ¶sterin. Ã–ÄŸrenciler quantization ile hafÄ±zanÄ±n 32GB'den 2GB'ye dÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ gÃ¶rÃ¼rler.

**Performans YarÄ±ÅŸÄ±**
LoRA vs DoRA vs PiSSA ile aynÄ± fine-tuning gÃ¶revlerini Ã§alÄ±ÅŸtÄ±rÄ±n. Ã–ÄŸrenciler doÄŸruluÄŸun gerÃ§ek zamanlÄ± iyileÅŸtiÄŸini, geleceÄŸin geÃ§miÅŸi yendiÄŸini izlerler.

> **ğŸ¯ Tam EÄŸitim Entegrasyonu**: Bu hikaydeki her teknik, Ã¶ÄŸrenme modÃ¼llerimizde uygulamalÄ± uygulama bulur. Ã–ÄŸrenciler bu atÄ±lÄ±mlarÄ± sadece Ã¶ÄŸrenmezlerâ€”yeniden yaratÄ±rlar, geliÅŸtirirler ve kendi yeniliklerini keÅŸfederler.

### Sonraki BÃ¶lÃ¼m: 2025'te Neler Geliyor

Ä°leriye bakarken, birkaÃ§ ortaya Ã§Ä±kan trend hikayemizi daha da heyecan verici hale vaat ediyor:

**Multimodal Devrim**
YakÄ±nda, sadece dil modellerini fine-tune etmeyeceksinizâ€”gÃ¶rme + dil sistemlerini birlikte uyarlayacaksÄ±nÄ±z. AynÄ± anda kod yazabilen VE UI ekran gÃ¶rÃ¼ntÃ¼lerini anlayabilen bir model eÄŸitmeyi hayal edin.

> **ğŸ¯ EÄŸitim Ã–nizlemesi**: `11-multimodal/` modÃ¼lÃ¼mÃ¼z K11'de LLaVA fine-tuning deneyimi yapmanÄ±zÄ± saÄŸlayacak, sadece yÄ±llar Ã¶nce imkansÄ±z gÃ¶rÃ¼nen ÅŸekillerde metin ve gÃ¶rÃ¼ntÃ¼ anlayÄ±ÅŸÄ±nÄ± birleÅŸtirecek.

**Federatif Gelecek** 
EÄŸitim daÄŸÄ±tÄ±k ve Ã¶zel hale gelecek. K11'iniz binlerce baÅŸka cihazla iÅŸ birliÄŸi yaparak tÃ¼m verileri yerel tutarken daha iyi modeller eÄŸitebilir.

**Kendini Ä°yileÅŸtiren Yapay Zeka**
Modeller kullanÄ±cÄ± geri bildirimlerinden sÃ¼rekli iyileÅŸecek, hatalarÄ± otomatik olarak dÃ¼zeltip manuel mÃ¼dahale olmadan yeni alanlara uyum saÄŸlayacaklar.

### En Ã–nemli Ders

Bu yolculuk boyuncaâ€”Hu'nun ilk LoRA deneylerinden 2024'Ã¼n son teknoloji tekniklerineâ€”bir kalÄ±p ortaya Ã§Ä±kÄ±yor: **en derin atÄ±lÄ±mlar genellikle baÅŸkalarÄ±nÄ±n gÃ¶rmezden geldiÄŸi basit sorularÄ± sormaktan gelir.**

- Hu sordu: "GerÃ§ekten TÃœM aÄŸÄ±rlÄ±klarÄ± gÃ¼ncellememiz gerekiyor mu?"
- Liu sordu: "LoRA aÄŸÄ±rlÄ±k yapÄ±sÄ±na aslÄ±nda ne yapÄ±yor?"
- PiSSA araÅŸtÄ±rmacÄ±larÄ± sordu: "Neden rastgele sayÄ±larla baÅŸlÄ±yoruz?"
- Answer.AI sordu: "DoRA neden quantization ile Ã§alÄ±ÅŸmasÄ±n?"

### Ã–ÄŸrencilerinizin FÄ±rsatÄ±

BugÃ¼n sÄ±nÄ±fÄ±nÄ±zda oturan Ã¶ÄŸrenciler, bu atÄ±lÄ±mlarÄ± yaratan araÃ§larÄ±n aynÄ±sÄ±na sahipler. OnlarÄ±n elinde:
- **Hugging Face aracÄ±lÄ±ÄŸÄ±yla son teknoloji modellere eriÅŸim**
- **GÃ¼Ã§lÃ¼ teknikler** (LoRA, DoRA, RLAIF) 
- **Uygun fiyatlÄ± donanÄ±m** (K11 gibi) her ÅŸeyi Ã§alÄ±ÅŸtÄ±rabilen
- **AÃ§Ä±k kaynak framework'ler** deneyleri demokratikleÅŸtiren

En Ã¶nemlisi, **taze gÃ¶zleri** var deneyimli araÅŸtÄ±rmacÄ±larÄ±n kaÃ§Ä±rdÄ±ÄŸÄ±nÄ± gÃ¶rebilecek.

### Maceraya Ã‡aÄŸrÄ±

*"Bu hikaydeki her teknik sizin gibi biriyle baÅŸladÄ±, imkansÄ±z gÃ¶rÃ¼nen bir problemle karÅŸÄ± karÅŸÄ±ya. Hu GPT-3'Ã¼ yeniden eÄŸitmeye gÃ¼cÃ¼ yetmiyordu. Liu LoRA'nÄ±n neden bu kadar iyi Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlayamÄ±yordu. Answer.AI ekibi verimlilik ve performansÄ±n karÅŸÄ±lÄ±klÄ± olarak mÃ¼nhasÄ±r olduÄŸunu kabul edemiyordu."*

*"SÄ°Z hangi imkansÄ±z problemi Ã§Ã¶zeceksiniz? Hangi basit soruyu sorup her ÅŸeyi deÄŸiÅŸtireceksiniz? Bu hikayenin sonraki bÃ¶lÃ¼mÃ¼ sizin yazmanÄ±zÄ± bekliyor."*

---

## ğŸ“š Tam Ã–ÄŸrenme Entegrasyonu

### **Tarih Boyunca UygulamalÄ± Yolculuk**

**AtÄ±lÄ±mlar Boyunca ModÃ¼l Yolu:**
- `00-first-time-beginner/`: "Ä°mkansÄ±z rÃ¼ya"nÄ±n mÃ¼mkÃ¼n hale geliÅŸini deneyimleyin
- `02-huggingface-peft/`: Hu'nun LoRA temelinde ustalaÅŸÄ±n  
- `10-cutting-edge-peft/`: Liu'nun DoRA atÄ±lÄ±mÄ±nÄ± uygulayÄ±n
- `08-llamafactory/`: HÄ±zlÄ± deneyim iÃ§in kod yazmadan arayÃ¼zler kullanÄ±n
- `12-advanced-rlhf/`: Anayasal Yapay Zeka ve RLAIF daÄŸÄ±tÄ±n
- `04-quantization/`: VerimliliÄŸi performansla birleÅŸtirin (QDoRA)

**Kodda Tam Hikaye YayÄ±:**
Ã–ÄŸrenciler bu atÄ±lÄ±mlar HAKKINDA sadece Ã¶ÄŸrenmezlerâ€”yolculuÄŸu yeniden yaratÄ±rlar, hayal kÄ±rÄ±klÄ±klarÄ±nÄ± deneyimlerler, aydÄ±nlanma anlarÄ±nÄ± kutlarlar ve sonraki bÃ¶lÃ¼mÃ¼ yaratmaya hazÄ±r olarak Ã§Ä±karlar.

### **AraÅŸtÄ±rma Makaleleri GerÃ§ek Oldu**
- **DoRA Makalesi**: [Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) - kendiniz uygulayÄ±n
- **PiSSA Ã‡alÄ±ÅŸmalarÄ±**: [Principal Singular Values Adaptation](https://arxiv.org/abs/2404.02948) - hÄ±z farkÄ±nÄ± gÃ¶rÃ¼n  
- **RLAIF AraÅŸtÄ±rmasÄ±**: [Constitutional AI papers](https://arxiv.org/abs/2212.08073) - kendi hizalanmÄ±ÅŸ modellerinizi eÄŸitin
- **LLaMA-Factory**: [Zero-code fine-tuning](https://github.com/hiyouga/LLaMA-Factory) - eriÅŸilebilirliÄŸin geleceÄŸini deneyimleyin
- **Unsloth**: [2x faster training](https://github.com/unslothai/unsloth) - hafÄ±za-verimli fine-tuning
- **AMD ROCm**: [GPU acceleration guide](https://rocm.docs.amd.com/) - donanÄ±mÄ±nÄ±zÄ± optimize edin

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§ Rehberi

### Ã–nkoÅŸullar
- **DonanÄ±m**: AMD Ryzen 9 8945HS + Radeon 780M (veya benzer)
- **RAM**: 32GB+ Ã¶nerilen
- **Depolama**: 100GB+ boÅŸ alanla hÄ±zlÄ± NVMe SSD

### Kurulum
```bash
# Bu repository'yi klonlayÄ±n
git clone https://github.com/your-username/fine_tuning.git
cd fine_tuning

# Yeni baÅŸlayan modÃ¼lÃ¼ ile baÅŸlayÄ±n
cd 00-first-time-beginner/
python test_setup.py  # OrtamÄ±nÄ±zÄ± doÄŸrulayÄ±n

# Veya kod yazmadan arayÃ¼ze atlayÄ±n
cd 08-llamafactory/
llamafactory-cli webui  # Web arayÃ¼zÃ¼nÃ¼ aÃ§ar
```

### Ã–ÄŸrenme Yolu
1. **ğŸ¯ BaÅŸlayÄ±n**: `00-first-time-beginner/` (Qwen2.5 0.6B, 30 dakika)
2. **âš¡ HÄ±z**: `01-unsloth/` (2x daha hÄ±zlÄ± eÄŸitim)
3. **ğŸ”§ Standart**: `02-huggingface-peft/` (LoRA temelleri)
4. **ğŸš€ Ä°leri**: `10-cutting-edge-peft/` (DoRA, PiSSA)
5. **ğŸ–¥ï¸ Kod Yazmadan**: `08-llamafactory/` (Web arayÃ¼zÃ¼)

---

## ğŸ“Š Repository Ä°statistikleri

![GitHub stars](https://img.shields.io/github/stars/your-username/fine_tuning?style=social)
![GitHub forks](https://img.shields.io/github/forks/your-username/fine_tuning?style=social)
![GitHub issues](https://img.shields.io/github/issues/your-username/fine_tuning)
![GitHub license](https://img.shields.io/github/license/your-username/fine_tuning)

**ğŸŒŸ Bu repository'yi yÄ±ldÄ±zlayÄ±n** eÄŸer fine-tuning devrimini anlamanÄ±zda yardÄ±mcÄ± olduysa!

**ğŸ”„ Fork edin ve katkÄ±da bulunun** - sonraki atÄ±lÄ±m sizin olabilir!

---

*Bu atÄ±lÄ±mlarÄ±n hikayesi sizin keÅŸif yolculuÄŸunuz haline gelir. Her teknik, her iÃ§gÃ¶rÃ¼, her atÄ±lÄ±m sadece akademik tarih deÄŸilâ€”Ã¼zerinde ustalaÅŸabileceÄŸiniz, geliÅŸtirebileceÄŸiniz ve sonunda devrim yaratabileceÄŸiniz pratik beceridir.*

**â¤ï¸ ile yaratÄ±ldÄ± [Beyhan MEYRALI](https://www.linkedin.com/in/beyhanmeyrali/) tarafÄ±ndan | Yapay zekanÄ±n demokratikleÅŸmesi iÃ§in optimize edildi**