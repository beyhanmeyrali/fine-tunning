# BÃ¼yÃ¼k Ä°nce Ayar Devrimi: Yapay ZekanÄ±n En Dramatik AtÄ±lÄ±mlarÄ±na Bir Yolculuk

[![Lisans](https://img.shields.io/badge/Lisans-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3.11+-green.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)
[![AMD ROCm](https://img.shields.io/badge/AMD-ROCm-red.svg)](https://rocm.docs.amd.com/)

> **ğŸ“ OluÅŸturan:** [Beyhan MEYRALI](https://www.linkedin.com/in/beyhanmeyrali/)  
> **ğŸ›ï¸ Optimize Edildi:** [GMKtec K11](https://www.gmktec.com/products/amd-ryzen%E2%84%A2-9-8945hs-nucbox-k11) ile AMD Ryzen 9 8945HS + Radeon 780M  
> **ğŸ“š Ã–ÄŸrenme YolculuÄŸu:** 15 dakikalÄ±k demolardan Ã¼retim daÄŸÄ±tÄ±mÄ±na

*Zeki beyinlerin, imkÃ¢nsÄ±z zorluklarÄ±n ve her ÅŸeyi deÄŸiÅŸtiren tekniklerin hikayesi*

---

## Ã–nsÃ¶z: Ä°mkÃ¢nsÄ±z RÃ¼ya

Åunu hayal edin: 2020 yÄ±lÄ±ndayÄ±z ve parlak bir fikriniz var. GPT-3'Ã¼ Ã¶zel bir gÃ¶rev iÃ§in Ã¶zelleÅŸtirmek istiyorsunuzâ€”belki eski dilleri Ã§evirmek ya da daha iyi kod yazmak. Ancak kÃ¼Ã§Ã¼k bir sorun var: GPT-3, 175 milyar parametreye sahip. SÄ±fÄ±rdan eÄŸitmek 12 milyon dolara mal olur ve bir sÃ¼per bilgisayar gerektirir.

DizÃ¼stÃ¼ bilgisayarÄ±nÄ±za bakÄ±yorsunuzâ€”belki **AMD Ryzen 9 8945HS ve Radeon 780M ile GMKtec K11, GPU gÃ¶revleri iÃ§in 8GB veya daha fazla paylaÅŸÄ±lan sistem belleÄŸi kullanan** mÃ¼tevazÄ± bir makineâ€”ve bu fikrin saÃ§malÄ±ÄŸÄ±na gÃ¼lÃ¼yorsunuz. Bu, bir oyuncak Ã§ekiÃ§le AltÄ±n KapÄ± KÃ¶prÃ¼sÃ¼'nÃ¼ yeniden inÅŸa etmeye Ã§alÄ±ÅŸmak gibi.

Ama ya size 2025 yÄ±lÄ±nda bu aynÄ± dizÃ¼stÃ¼ bilgisayarÄ±n GPT-3'ten daha gÃ¼Ã§lÃ¼ modelleri ince ayar yapabileceÄŸini sÃ¶ylesem? Ya imkÃ¢nsÄ±z sadece mÃ¼mkÃ¼n deÄŸil, aynÄ± zamanda *kolay* hale gelseydi?

Bu, birkaÃ§ zeki araÅŸtÄ±rmacÄ±nÄ±n daÄŸlarÄ± yerinden oynatmadÄ±ÄŸÄ±, aslÄ±nda onlarÄ± hareket ettirmeye hiÃ§ gerek olmadÄ±ÄŸÄ±nÄ± Ã¶ÄŸrettiÄŸi hikaye.

---

## BÃ¶lÃ¼m 1: Temel - Microsoft Her Åeyi DeÄŸiÅŸtirdiÄŸinde

### Edward Huâ€™nun DehasÄ±

2021 yÄ±lÄ±nda Microsoft Researchâ€™Ã¼n koridorlarÄ±nda Edward Hu, yapay zeka topluluÄŸunu yÄ±llardÄ±r ÅŸaÅŸÄ±rtan bir matematiksel bulmacayla boÄŸuÅŸuyordu. Devasa bir sinir aÄŸÄ±nÄ± yeniden eÄŸitmeden ona yeni numaralar nasÄ±l Ã¶ÄŸretilirdi?

Geleneksel bilgiye gÃ¶re, tÃ¼m aÄŸÄ±rlÄ±klarÄ±â€”GPT-3 iÃ§in 175 milyar tanesiniâ€”gÃ¼ncellemek zorundaydÄ±nÄ±z. Bu, yeni bir tablo asmak iÃ§in evinizi tamamen yeniden inÅŸa etmeniz gerektiÄŸini sÃ¶ylemek gibiydi.

Ama Huâ€™nun farklÄ± bir fikri vardÄ±. Ya yapÄ±lmasÄ± gereken â€œdeÄŸiÅŸikliklerâ€ aslÄ±nda o kadar karmaÅŸÄ±k deÄŸilse? Ya bilginin Ã§oÄŸu zaten oradaysa ve sadece birkaÃ§ stratejik â€œadaptÃ¶râ€ eklemek gerekiyorsa?

**AtÄ±lÄ±m AnÄ±**

Hu, derin bir gerÃ§eÄŸi fark etti: Ä°nce ayar iÃ§in gereken gÃ¼ncellemeler genellikle dÃ¼ÅŸÃ¼k iÃ§sel boyuta sahiptir. BasitÃ§e sÃ¶ylemek gerekirse, yapÄ±lmasÄ± gereken deÄŸiÅŸiklikler Ã§ok daha kÃ¼Ã§Ã¼k matematiksel yapÄ±larla ifade edilebilir.

Devasa bir W matrisini doÄŸrudan gÃ¼ncellemek yerine:
```python
W_new = W + Î”W  # Î”W bÃ¼yÃ¼k ve pahalÄ±
```

GÃ¼ncellemeyi iki kÃ¼Ã§Ã¼k matrise ayÄ±rdÄ±:
```python
W_new = W + B Ã— A  # B ve A Ã§ok, Ã§ok daha kÃ¼Ã§Ã¼k
```

Bu, bir ansiklopediyi tamamen yeniden yazmak yerine, doÄŸru deÄŸiÅŸiklikleri iÅŸaret eden kÃ¼Ã§Ã¼k bir dizin eklemek gibiydi.

**Sihirli SayÄ±lar**

Hu, LoRAâ€™yÄ± (DÃ¼ÅŸÃ¼k Rank Adaptasyonu) yayÄ±nladÄ±ÄŸÄ±nda sonuÃ§lar ÅŸaÅŸÄ±rtÄ±cÄ±ydÄ±:
- **Bellek kullanÄ±mÄ± %95 azaldÄ±** - bazÄ± modeller iÃ§in 32GBâ€™dan 1.5GBâ€™a
- **EÄŸitim sÃ¼resi %90 dÃ¼ÅŸtÃ¼** - gÃ¼nlerden saatlere
- **Performans aynÄ± kaldÄ±** - kalitede hiÃ§ kayÄ±p yok

Aniden, masanÄ±zdaki GMKtec K11 bir oyuncak olmaktan Ã§Ä±ktÄ±. GerÃ§ek bir yapay zeka araÅŸtÄ±rma istasyonu haline geldi.

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: Ä°ÅŸte bu yÃ¼zden `02-huggingface-peft/` modÃ¼lÃ¼mÃ¼z LoRA ile baÅŸlÄ±yor - bu, diÄŸer her ÅŸeyin mÃ¼mkÃ¼n olmasÄ±nÄ± saÄŸlayan temel. K11â€™de ilk LoRA ince ayarÄ±nÄ±zÄ± Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda, Huâ€™nun izinden gidiyorsunuz.

**ğŸ“š Daha Fazla Bilgi:**
- **Orijinal Makale**: [LoRA: BÃ¼yÃ¼k Dil Modeli Adaptasyonu](https://arxiv.org/abs/2106.09685)
- **HuggingFace PEFT**: [https://github.com/huggingface/peft](https://github.com/huggingface/peft)
- **Microsoft AraÅŸtÄ±rma Blogu**: [LoRA: BÃ¼yÃ¼k Dil Modellerini Uyarlama](https://www.microsoft.com/en-us/research/blog/lora-adapting-large-language-models/)

---

## BÃ¶lÃ¼m 2: NVIDIA Devrimi - Ä°yi, Harika OlduÄŸunda

### LoRAâ€™yÄ± GÃ¶lgeleyen Gizem

ÃœÃ§ yÄ±l boyunca LoRA, etkili ince ayarlarÄ±n tartÄ±ÅŸmasÄ±z kralÄ±ydÄ±. DÃ¼nya Ã§apÄ±ndaki araÅŸtÄ±rmacÄ±lar onu kullandÄ±, sevdi ve kariyerlerini onun Ã¼zerine inÅŸa etti. Ancak NVIDIAâ€™dan Shih-Yang Liuâ€™nun iÃ§ini kemiren bir his vardÄ±.

LoRA inanÄ±lmaz derecede iyi Ã§alÄ±ÅŸÄ±yordu, ama *neden*? Ve daha da Ã¶nemlisi, naber eksik?

Liu, sinir aÄŸÄ± aÄŸÄ±rlÄ±klarÄ±nÄ±n matematiÄŸine derinlemesine daldÄ±. SayÄ±lara bakmÄ±yordu sadeceâ€”onlarÄ±n *Ã¶zÃ¼nÃ¼* anlamaya Ã§alÄ±ÅŸÄ±yordu. Bir aÄŸÄ±rlÄ±k matrisini ne harekete geÃ§iriyordu?

**Eureka AnÄ±**

2024â€™te bir akÅŸam, Liuâ€™nun atÄ±lÄ±mÄ± geldi. Her aÄŸÄ±rlÄ±k matrisinin iki temel bileÅŸeni olduÄŸunu fark etti:
- **BÃ¼yÃ¼klÃ¼k**: BaÄŸlantÄ±nÄ±n ne kadar â€œgÃ¼Ã§lÃ¼â€ olduÄŸu
- **YÃ¶n**: BaÄŸlantÄ±nÄ±n hangi yÃ¶ne iÅŸaret ettiÄŸi

Bu, fizikte bir vektÃ¶rÃ¼ tanÄ±mlamak gibiydiâ€”hem gÃ¼cÃ¼ hem de yÃ¶nÃ¼ tam olarak anlamak iÃ§in ihtiyacÄ±nÄ±z var.

Sonra ÅŸok edici bir gerÃ§ek ortaya Ã§Ä±ktÄ±: **LoRA sadece yÃ¶nÃ¼ uyarlÄ±yordu!**

```python
# LoRAâ€™nÄ±n aslÄ±nda yaptÄ±ÄŸÄ± (farkÄ±nda olmadan)
W = bÃ¼yÃ¼klÃ¼k Ã— yÃ¶n
LoRA_gÃ¼ncellemesi = sadece_yÃ¶n_deÄŸiÅŸikliÄŸi  # Resmin yarÄ±sÄ± eksik!

# DoRAâ€™nÄ±n Ã¶nerdiÄŸi
W = bÃ¼yÃ¼klÃ¼k Ã— yÃ¶n  
DoRA_gÃ¼ncellemesi = bÃ¼yÃ¼klÃ¼k_deÄŸiÅŸikliÄŸi + yÃ¶n_deÄŸiÅŸikliÄŸi  # Tam resim!
```

**Her Åeyi DeÄŸiÅŸtiren Deney**

Liu ve ekibi DoRAâ€™yÄ± (AÄŸÄ±rlÄ±k AyrÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ DÃ¼ÅŸÃ¼k Rank Adaptasyonu) test ettiÄŸinde, sonuÃ§lar nefes kesiciydi:

- **Llama 7B, akÄ±l yÃ¼rÃ¼tme gÃ¶revlerinde 3.7 puan arttÄ±**
- **Llama 3 8B, performansta 4.4 puan sÄ±Ã§radÄ±**
- **Test edilen her model** tutarlÄ± iyileÅŸmeler gÃ¶sterdi
- **Ek bellek maliyeti yok** - LoRA ile aynÄ± verimlilik

Bu, sanatÃ§Ä±larÄ±n tÃ¼m bu sÃ¼re boyunca paletlerinin sadece yarÄ±sÄ±nÄ± kullanarak resim yaptÄ±ÄŸÄ±nÄ± keÅŸfetmek gibiydi.

**GerÃ§ek DÃ¼nya Etkisi**

SayÄ±lar hikayeyi anlatÄ±yor:
- **SaÄŸduyu akÄ±l yÃ¼rÃ¼tme**: +3.7 puan (AIâ€™da bu Ã§ok bÃ¼yÃ¼k!)
- **Ã‡ok turlu sohbetler**: +0.4 puan (daha doÄŸal diyalog)
- **GÃ¶rsel gÃ¶revler**: +1.9 puan (daha iyi gÃ¶rÃ¼ntÃ¼ anlama)

BaÄŸlam iÃ§in, AI kÄ±yaslamalarÄ±nda 1 puanlÄ±k bir iyileÅŸme Ã¶nemli kabul edilir. DoRA, tutarlÄ± bir ÅŸekilde 3-4 puanlÄ±k sÄ±Ã§ramalar sunuyordu.

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: DoRA, `06-advanced-techniques/` modÃ¼lÃ¼mÃ¼zÃ¼n yÄ±ldÄ±zÄ±. K11â€™de LoRA ile DoRA performansÄ±nÄ± karÅŸÄ±laÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda, bu iyileÅŸmeleri bizzat gÃ¶receksiniz. Ä°yi ile harika ince ayar arasÄ±ndaki fark bu.

**ğŸ“š Daha Fazla Bilgi:**
- **Orijinal Makale**: [DoRA: AÄŸÄ±rlÄ±k AyrÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ DÃ¼ÅŸÃ¼k Rank Adaptasyonu](https://arxiv.org/abs/2402.09353)
- **NVIDIA GeliÅŸtirici Blogu**: [DoRAâ€™yÄ± TanÄ±tÄ±yoruz, LoRAâ€™ya YÃ¼ksek PerformanslÄ± Bir Alternatif](https://developer.nvidia.com/blog/introducing-dora-a-high-performing-alternative-to-lora-for-fine-tuning/)
- **DoRA UygulamasÄ±**: [NVlabs/DoRA](https://github.com/NVlabs/DoRA)

---

## BÃ¶lÃ¼m 3: TopluluÄŸun CevabÄ± - Zeki Beyinler Ä°ÅŸbirliÄŸi YaptÄ±ÄŸÄ±nda

### MÃ¼kemmeliyetin Sorunu

DoRA harikaydÄ±, ancak bir sorunu vardÄ±: LoRA ile aynÄ± belleÄŸi gerektiriyordu. GMKtec K11 gibi mÃ¼tevazÄ± donanÄ±mlara sahip birÃ§ok araÅŸtÄ±rmacÄ± iÃ§inâ€”GPU gÃ¶revleri iÃ§in 8GB veya daha fazla paylaÅŸÄ±lan sistem belleÄŸi ileâ€”daha bÃ¼yÃ¼k modellerle Ã§alÄ±ÅŸÄ±rken LoRA bile zorlayÄ±cÄ± olabiliyordu.

Answer.AI, yapay zekanÄ±n sadece milyon dolarlÄ±k GPU kÃ¼melerine sahip olanlara deÄŸil, herkese eriÅŸilebilir olmasÄ± gerektiÄŸine inanan yenilikÃ§i bir araÅŸtÄ±rma laboratuvarÄ± olarak devreye girdi.

### Ä°ki Zeki Fikrin EvliliÄŸi

Answer.AI ekibi DoRAâ€™ya bakarak Ã§Ä±lgÄ±nca bir fikir ortaya attÄ±: â€œYa bunu kuantizasyonla birleÅŸtirirsek?â€

Kuantizasyon, bilmeyenler iÃ§in, Ã¶nemli detaylarÄ± kaybetmeden yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ bir fotoÄŸrafÄ± sÄ±kÄ±ÅŸtÄ±rmak gibidir. SayÄ±larÄ± 16 bit hassasiyetle saklamak yerine, genellikle sadece 4 bit ile yetinebilirsinizâ€”bu, 4 kat bellek azalmasÄ± demek!

Sorun ÅŸuydu: Kuantizasyon her zaman LoRA ile eÅŸleÅŸtirilmiÅŸti. Kimse bunu Ã¼stÃ¼n DoRA ile denememiÅŸti.

**Topluluk Deneyi**

Sonra olanlar gÃ¼zeldi. Answer.AI, QDoRAâ€™yÄ± (Kuantize EdilmiÅŸ DoRA) tek baÅŸÄ±na geliÅŸtirmedi. Toplulukla iÅŸbirliÄŸi yaptÄ±, deneyleri paylaÅŸtÄ±, geri bildirim topladÄ± ve hÄ±zla yineledi.

SonuÃ§? **QDoRA**â€”size ÅŸunu veren bir teknik:
- **DoRAâ€™nÄ±n Ã¼stÃ¼n performansÄ±** (LoRAâ€™ya gÃ¶re +3.7 puan)
- **Kuantizasyonun bellek verimliliÄŸi** (4 kat bellek azalmasÄ±)
- **Tek bir teknikte her iki dÃ¼nyanÄ±n en iyisi**

```python
# Sihirli kombinasyon
QDoRA = DoRA_performansÄ± + Kuantizasyon_verimliliÄŸi
# SonuÃ§: 1/4 bellekte Ã¼stÃ¼n adaptasyon
```

**K11 BaÄŸlantÄ±sÄ±**

Bu, GMKtec K11 gibi donanÄ±mlar iÃ§in oyunun kurallarÄ±nÄ± deÄŸiÅŸtirdi. Aniden ÅŸunlarÄ± yapabilirdiniz:
- **7B modelleri 8GB paylaÅŸÄ±lan sistem belleÄŸinde Ã§alÄ±ÅŸtÄ±rÄ±n** (Ã¶nceden imkÃ¢nsÄ±zdÄ±)
- **DoRA seviyesinde performans** kuantizasyon verimliliÄŸiyle alÄ±n
- **Tam ince ayardan daha iyi performans gÃ¶steren modeller eÄŸitin**
- Hepsi bir tÃ¼ketici masaÃ¼stÃ¼nde

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: QDoRA, `04-quantization/` ve `06-advanced-techniques/` modÃ¼llerimizde Ã¶ne Ã§Ä±kÄ±yor. CLAUDE.mdâ€™de â€œEXTREME_CONFIGâ€ (1 toplu iÅŸ boyutu, 4-bit kuantizasyon) gÃ¶rdÃ¼ÄŸÃ¼nÃ¼zde, K11â€™de QDoRAâ€™yÄ± mÃ¼mkÃ¼n kÄ±lan tam olarak bu bellek-verimli teknikleri kullanÄ±yorsunuz.

**ğŸ“š Daha Fazla Bilgi:**
- **Answer.AI Blogu**: [QDoRA: Kuantize EdilmiÅŸ DoRA Ä°nce Ayar](https://www.answer.ai/posts/2024-03-14-qdora.html)
- **Topluluk TartÄ±ÅŸmasÄ±**: [QDoRA Uygulama Konusu](https://github.com/huggingface/peft/discussions/1474)
- **Kuantizasyon KÄ±lavuzu**: [BitsAndBytes DokÃ¼mantasyonu](https://github.com/TimDettmers/bitsandbytes)

---

## BÃ¶lÃ¼m 4: MatematikÃ§inin Ä°Ã§gÃ¶rÃ¼sÃ¼ - Neden Rastgele YanlÄ±ÅŸ?

### Her Åeyi BaÅŸlatan Can SÄ±kÄ±cÄ± Soru

DoRA manÅŸetlere Ã§Ä±karken, baÅŸka bir grup araÅŸtÄ±rmacÄ± rahatsÄ±z edici bir soru soruyordu: â€œNeden LoRA adaptÃ¶rlerini rastgele sayÄ±larla baÅŸlatÄ±yoruz?â€

DÃ¼ÅŸÃ¼nÃ¼n. BÃ¼yÃ¼k modelleri insan bilgisini kodlamak iÃ§in muazzam Ã§aba harcÄ±yoruz. Bu modeller, milyarlarca metin Ã¶rneÄŸinden Ã¶ÄŸrenilen desenleri iÃ§eriyor. AÄŸÄ±rlÄ±k matrisleri dilin, akÄ±l yÃ¼rÃ¼tmenin ve bilginin sÄ±rlarÄ±nÄ± tutuyor.

Ve sonra, onlarÄ± uyarlamak istediÄŸimizde... tamamen rastgele gÃ¼rÃ¼ltÃ¼yle mi baÅŸlÄ±yoruz?

Bu, bir usta ÅŸefin mÃ¼kemmel hazÄ±rlanmÄ±ÅŸ tarifine sahip olmak ve sonra marketten rasgele malzemeler seÃ§erek eklemeler yapmak gibiydi.

### Ana BileÅŸen Devrimi

PiSSA (Ana Tekil DeÄŸerler ve Tekil VektÃ¶rler Adaptasyonu) arkasÄ±ndaki matematikÃ§iler daha iyi bir fikir buldu. Rastgele baÅŸlatma yerine, modelin zaten bildiÄŸi **en Ã¶nemli kÄ±sÄ±mlardan** baÅŸlasak ne olur?

**Matematiksel Sihir**

Her aÄŸÄ±rlÄ±k matrisi, Tekil DeÄŸer AyrÄ±ÅŸÄ±mÄ± (SVD) kullanÄ±larak ayrÄ±ÅŸtÄ±rÄ±labilir. Bunu, karmaÅŸÄ±k bir senfoniyi en Ã¶nemli mÃ¼zikal temalarÄ±na ayÄ±rmak gibi dÃ¼ÅŸÃ¼nÃ¼n:

```python
# Geleneksel LoRA: Rastgele gÃ¼rÃ¼ltÃ¼yle baÅŸla
A = rastgele_gÃ¼rÃ¼ltÃ¼()  # Herhangi bir yere iÅŸaret edebilir!
B = sÄ±fÄ±rlar()         # BaÅŸlangÄ±Ã§ta hiÃ§bir katkÄ± saÄŸlamaz

# PiSSA: Modelin en Ã¶nemli desenleriyle baÅŸla
U, S, V = SVD(orijinal_aÄŸÄ±rlÄ±k)  # â€œSenfoniyiâ€ ayÄ±r
A = en_Ã¶nemli_U            # Ana â€œtemalarlaâ€ baÅŸla  
B = en_Ã¶nemli_S @ en_Ã¶nemli_V  # OnlarÄ±n Ã¶nem aÄŸÄ±rlÄ±klarÄ±
```

**Parlak Ä°Ã§gÃ¶rÃ¼**

PiSSA araÅŸtÄ±rmacÄ±larÄ±, en yÃ¼ksek tekil deÄŸerlerin ve vektÃ¶rlerin modelin Ã¶ÄŸrendiÄŸi en kritik desenleri temsil ettiÄŸini fark etti. AdaptÃ¶rleri bu bileÅŸenlerle baÅŸlatarak, ince ayar sÃ¼recine ÅŸunu sÃ¶ylÃ¼yorsunuz: â€œBuradan baÅŸlaâ€”en Ã§ok bu Ã¶nemli.â€

Bu, bir Ã¶ÄŸrenciye yeni materyali Ã¶ÄŸrenmeden Ã¶nce ders kitabÄ±nÄ±n en Ã¶nemli bÃ¶lÃ¼mlerini vermek gibiydi.

### Herkesi Åok Eden SonuÃ§lar

PiSSA test edildiÄŸinde, iyileÅŸmeler anÄ±nda ve tutarlÄ±ydÄ±:

- **Daha hÄ±zlÄ± yakÄ±nsama**: Modeller yeni gÃ¶revleri daha az adÄ±mda Ã¶ÄŸrendi
- **Daha iyi kararlÄ±lÄ±k**: EÄŸitim daha pÃ¼rÃ¼zsÃ¼z ve Ã¶ngÃ¶rÃ¼lebilir oldu
- **ÃœstÃ¼n nihai performans**: SonuÃ§lar sÃ¼rekli olarak rastgele baÅŸlatmayÄ± geÃ§ti
- **Ä°lkeli yaklaÅŸÄ±m**: Nihayet matematiksel olarak saÄŸlam bir adaptasyon baÅŸlangÄ±cÄ±

**Ã–ÄŸrenme HÄ±zÄ± Devrimi**

Belki de en Ã¶nemlisi, PiSSA modelleri daha hÄ±zlÄ± Ã¶ÄŸrendi. K11â€™deki eÄŸitim ortamÄ±mÄ±zda bu ÅŸu anlama geliyor:
- **Daha kÄ±sa eÄŸitim sÃ¼releri** (45-60 dakika yerine 15-30 dakika)
- **Daha az elektrik maliyeti** (uzun eÄŸitim oturumlarÄ± iÃ§in Ã¶nemli)
- **Daha hÄ±zlÄ± deney dÃ¶ngÃ¼leri** (aynÄ± sÃ¼rede daha fazla fikir deneyin)

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: PiSSA teknikleri `06-advanced-techniques/` modÃ¼lÃ¼mÃ¼ze entegre edilmiÅŸtir. EÄŸitimlerde baÅŸlatma stratejilerini karÅŸÄ±laÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda, â€œakÄ±llÄ±â€ baÅŸlamanÄ±n â€œrastgeleâ€ baÅŸlamayÄ± her zaman yendiÄŸini gÃ¶receksiniz. `00-first-time-beginner/` iÃ§indeki Mistral Large 2 Ã¶rneklerimizde bu Ã¶zellikle belirginâ€”rastgele baÅŸlatma ile 30 dakika sÃ¼ren aynÄ± model, PiSSA ile 15 dakikada yakÄ±nsar!

**ğŸ“š Daha Fazla Bilgi:**
- **AraÅŸtÄ±rma Makalesi**: [PiSSA: Ana Tekil DeÄŸerler ve VektÃ¶rler Adaptasyonu](https://arxiv.org/abs/2404.02948)
- **Uygulama**: [GraphPKU/PiSSA](https://github.com/GraphPKU/PiSSA)
- **Matematiksel Arka Plan**: [Tekil DeÄŸer AyrÄ±ÅŸÄ±mÄ± AÃ§Ä±klamasÄ±](https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm)

---

## BÃ¶lÃ¼m 5: Hizalanma Devrimi - Yapay Zeka Kendi Ã–ÄŸretmeni OlduÄŸunda

### Ä°nsan DarboÄŸazÄ± Sorunu

2024â€™e gelindiÄŸinde, yapay zeka geliÅŸtirmede yeni bir kriz ortaya Ã§Ä±kÄ±yordu. Modeller inanÄ±lmaz derecede gÃ¼Ã§lÃ¼ hale geliyordu, ancak onlarÄ± yardÄ±msever, zararsÄ±z ve dÃ¼rÃ¼st yapmak iÃ§in Ä°nsan Geri Bildirimiyle PekiÅŸtirme Ã–ÄŸrenimi (RLHF) adÄ± verilen bir ÅŸey gerekiyordu.

SÃ¼reÃ§ basitti ama pahalÄ±ydÄ±: Ä°nsanlara binlerce model yanÄ±tÄ±nÄ± gÃ¶sterin, hangilerinin daha iyi olduÄŸunu deÄŸerlendirmelerini isteyin ve bu geri bildirimi modeli iyi davranmaya eÄŸitmek iÃ§in kullanÄ±n.

Sorun ÅŸuydu: **Ä°nsanlar pahalÄ±, yavaÅŸ ve tutarsÄ±z.**

Tek bir hizalanma Ã§alÄ±ÅŸmasÄ±, insan etiketleme Ã¼cretleri iÃ§in 50.000 dolara mal olabilirdi. Daha kÃ¶tÃ¼sÃ¼, insanlar yoruluyor, birbirleriyle Ã§eliÅŸiyor ve 7/24 Ã§alÄ±ÅŸamÄ±yordu. Bu, her Ã¶ÄŸrencinin kiÅŸisel bir Ã¶ÄŸretmene ihtiyaÃ§ duyarak eÄŸitimi Ã¶lÃ§eklendirmeye Ã§alÄ±ÅŸmak gibiydiâ€”teoride asil, pratikte imkÃ¢nsÄ±z.

### Anayasal Yapay Zeka AtÄ±lÄ±mÄ±

Anthropicâ€™ten (Claudeâ€™un arkasÄ±ndaki ÅŸirket) Lee ve meslektaÅŸlarÄ± devrimci bir fikir geliÅŸtirdi: Ya yapay zeka kendi kendine Ã¶ÄŸretmeyi Ã¶ÄŸrenseydi?

Anayasal Yapay Zeka dedikleri bir ÅŸey geliÅŸtirdiler; burada insanlar geri bildirim saÄŸlamak yerine, gÃ¼Ã§lÃ¼ bir yapay zeka modeli (GPT-4 gibi) bir â€œanayasaâ€ ilkelerine gÃ¶re yanÄ±tlarÄ± deÄŸerlendiriyor ve geliÅŸtiriyor.

**RLAIFâ€™in Sihri**

RLAIF (Yapay Zeka Geri Bildirimiyle PekiÅŸtirme Ã–ÄŸrenimi) senaryoyu tamamen tersine Ã§evirdi:

```python
# Geleneksel RLHF: PahalÄ± ve yavaÅŸ
insan_deÄŸerlendirmesi = pahalÄ±_insan_etiketleyici(model_yanÄ±tÄ±)
model.Ã¶ÄŸren(insan_deÄŸerlendirmesi)

# RLAIF: HÄ±zlÄ± ve Ã¶lÃ§eklenebilir
ai_deÄŸerlendirmesi = gpt4_anayasal_deÄŸerlendirici(model_yanÄ±tÄ±, anayasa)
model.Ã¶ÄŸren(ai_deÄŸerlendirmesi)
```

**Åok Edici SonuÃ§lar**

Diyalog ve Ã¶zetleme gÃ¶revlerinde test edildiÄŸinde, RLAIF modelleri RLHF modelleriyle **aynÄ±** performansÄ± gÃ¶sterdi. Yapay zeka geri bildirimi, insan geri bildirimi kadar iyiydi, ancak:

- **1000 kat daha ucuz**: Ã‡alÄ±ÅŸma baÅŸÄ±na 50.000 dolar yerine 500 dolar
- **1000 kat daha hÄ±zlÄ±**: Haftalar yerine dakikalar
- **Sonsuz Ã¶lÃ§eklenebilir**: Ä°nsan yorgunluÄŸu veya eriÅŸilebilirlik kÄ±sÄ±tlamalarÄ± yok
- **Daha tutarlÄ±**: Yapay zeka kÃ¶tÃ¼ gÃ¼nler geÃ§irmez veya anlaÅŸmazlÄ±k yaÅŸamaz

### Anayasal EÄŸitim Devrimi

AtÄ±lÄ±m, sadece maliyet tasarrufunun Ã¶tesine geÃ§ti. Anayasal Yapay Zeka, karmaÅŸÄ±k etik ilkeleri doÄŸrudan eÄŸitime kodlamayÄ± mÃ¼mkÃ¼n kÄ±ldÄ±:

- **YardÄ±mseverlik**: DÃ¼rÃ¼st kalarak maksimum yardÄ±m saÄŸlama
- **ZararsÄ±zlÄ±k**: ZararlÄ± iÃ§erik Ã¼retmekten kaÃ§Ä±nma
- **DÃ¼rÃ¼stlÃ¼k**: HalÃ¼sinasyon yerine belirsizliÄŸi kabul etme

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: Bu, `07-system-prompt-modification/` modÃ¼lÃ¼mÃ¼zÃ¼n temelidir. K11â€™de anayasal eÄŸitim kullanarak hizalanmÄ±ÅŸ modeller oluÅŸturduÄŸunuzda, bÃ¼yÃ¼k yapay zeka laboratuvarlarÄ±nÄ±n kullandÄ±ÄŸÄ± teknikleri kullanÄ±yorsunuzâ€”ama sizin Ã¶zel ihtiyaÃ§larÄ±nÄ±za uyarlanmÄ±ÅŸ. CLAUDE.mdâ€™de belirtilen â€œ20-40 dakika eÄŸitim sÃ¼resiâ€? Bu, RLAIFâ€™in tÃ¼ketici donanÄ±mÄ±nda geliÅŸmiÅŸ hizalamayÄ± eriÅŸilebilir kÄ±lmasÄ±.

**ğŸ“š Daha Fazla Bilgi:**
- **Anayasal Yapay Zeka Makalesi**: [Anayasal Yapay Zeka: Yapay Zeka Geri Bildiriminden ZararsÄ±zlÄ±k](https://arxiv.org/abs/2212.08073)
- **RLAIF AraÅŸtÄ±rmasÄ±**: [RLAIF: Ä°nsan Geri Bildiriminden PekiÅŸtirme Ã–ÄŸrenimini Ã–lÃ§eklendirme](https://arxiv.org/abs/2309.00267)
- **Anthropic Blogu**: [Anayasal Yapay Zeka: Yapay Zeka Geri Bildiriminden ZararsÄ±zlÄ±k](https://www.anthropic.com/news/constitutional-ai-harmlessness-from-ai-feedback)
- **Uygulama KÄ±lavuzu**: [TRL Anayasal EÄŸitim](https://huggingface.co/docs/trl/constitutional_ai)

---

## BÃ¶lÃ¼m 6: LLaMA-Factory Devrimi - KarmaÅŸÄ±klÄ±k Basit OlduÄŸunda

### Ã‡ok Fazla SeÃ§enek Sorunu

2024â€™e gelindiÄŸinde, ince ayar manzarasÄ± inanÄ±lmaz derecede zenginâ€”ve inanÄ±lmaz derecede kafa karÄ±ÅŸtÄ±rÄ±cÄ±â€”hale gelmiÅŸti. LoRA, DoRA, QLoRA, QDoRA, PiSSA, RLHF, RLAIF ve daha onlarca teknik vardÄ±. Her biri gÃ¼Ã§lÃ¼ydÃ¼, ama hepsini Ã¶ÄŸrenmek, dÃ¼nyadaki her mutfaÄŸÄ± Ã¶ÄŸrenerek usta ÅŸef olmaya Ã§alÄ±ÅŸmak gibiydi.

LLaMA-Factory, basit bir soru soran bir proje olarak devreye girdi: â€œYa tÃ¼m bu teknikleri tek bir gÃ¼zel arayÃ¼z Ã¼zerinden eriÅŸilebilir kÄ±lsak?â€

### SÄ±fÄ±r Kod Devrimi

LLaMA-Factoryâ€™nin yaratÄ±cÄ±larÄ± derin bir gerÃ§eÄŸi fark etti: Modelleri ince ayar yapmak isteyen herkes programcÄ± deÄŸil. Doktorlar, avukatlar, araÅŸtÄ±rmacÄ±lar, yazarlar ve giriÅŸimciler, yapay zekayÄ± iyileÅŸtirebilecek alan bilgisine sahipâ€”ama Python, CUDA ve daÄŸÄ±tÄ±k eÄŸitimi Ã¶ÄŸrenmeleri gerekmemeli.

**Web ArayÃ¼zÃ¼ AtÄ±lÄ±mÄ±**

LLaMA-Factory, devrimci bir ÅŸey sundu: Ä°nce ayar iÃ§in bir web arayÃ¼zÃ¼. ÅunlarÄ± yapabilirdiniz:
- **100â€™den fazla model arasÄ±ndan seÃ§im yapÄ±n**, Mistral Large 2 gibi en son sÃ¼rÃ¼mler dahil
- **EÄŸitim yÃ¶nteminizi seÃ§in** (LoRA, DoRA, RLHF) aÃ§Ä±lÄ±r menÃ¼lerden
- **Verilerinizi yÃ¼kleyin** dosyalarÄ± sÃ¼rÃ¼kleyip bÄ±rakarak
- **EÄŸitimi izleyin** gerÃ§ek zamanlÄ± grafiklerle
- **Modelleri dÄ±ÅŸa aktarÄ±n** ihtiyacÄ±nÄ±z olan herhangi bir formata

```bash
# Her ÅŸeyi deÄŸiÅŸtiren sihirli komut
llamafactory-cli webui
# TarayÄ±cÄ±nÄ±zda gÃ¼zel bir arayÃ¼z aÃ§ar
```

**GÃ¼n-0 Vaadi**

Belki de en dikkat Ã§ekici olanÄ±, LLaMA-Factoryâ€™nin yeni modeller iÃ§in â€œGÃ¼n-0â€ desteÄŸi taahhÃ¼t etmesiydi. Meta, Llama 3.1â€™i yayÄ±nladÄ±ÄŸÄ±nda, LLaMA-Factoryâ€™de saatler iÃ§inde destekleniyordu. Mistral Large 2 piyasaya sÃ¼rÃ¼ldÃ¼ÄŸÃ¼nde, hemen oradaydÄ±.

Bu sadece kolaylÄ±k deÄŸildiâ€”devrimdi. Daha Ã¶nce, Ã§erÃ§eve desteÄŸi iÃ§in 3-6 ay beklemek normaldi. LLaMA-Factory, en yeni modelleri anÄ±nda eriÅŸilebilir kÄ±ldÄ±.

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: Ä°ÅŸte bu yÃ¼zden Ã¶ÄŸrenme yolculuÄŸumuza `08-llamafactory/` ekledik. 00-07 modÃ¼llerinde temelleri Ã¶ÄŸrendikten sonra, LLaMA-Factory geliÅŸmiÅŸ deneyler iÃ§in â€œkomuta merkeziâ€ olur. Web arayÃ¼zÃ¼, yapÄ±landÄ±rma dosyalarÄ± yerine veri ve sonuÃ§lara odaklanmak isteyen Ã¶ÄŸrenciler iÃ§in mÃ¼kemmel.

### Ãœretim HattÄ± RÃ¼yasÄ±

Ama LLaMA-Factory daha ileri gitti. Ä°nce ayarÄ± kolaylaÅŸtÄ±rmakla yetinmediâ€”onu **tamamlanmÄ±ÅŸ** yaptÄ±. Yeni baÅŸlayanlara yardÄ±mcÄ± olan aynÄ± araÃ§ ÅŸunlarÄ± da destekledi:
- **Ã‡oklu GPU eÄŸitimi** DeepSpeed entegrasyonu ile
- **Deney takibi** Wandb ve TensorBoard ile
- **Model deÄŸerlendirme** kapsamlÄ± kÄ±yaslamalarla
- **DaÄŸÄ±tÄ±m hatlarÄ±** vLLM ve Ollama ihracatÄ± ile

Bu, basit bir hesap makinesine sahip olup gerektiÄŸinde diferansiyel denklemleri Ã§Ã¶zebilen bir makineye dÃ¶nÃ¼ÅŸmesi gibiydi.

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: `03-ollama/` modÃ¼lÃ¼mÃ¼z, LLaMA-Factoryâ€™yi tamamlar ve ince ayar yapÄ±lmÄ±ÅŸ modelleri yerel veya uÃ§ cihazlarda Ã§alÄ±ÅŸtÄ±rmak iÃ§in daÄŸÄ±tÄ±m komut dosyalarÄ± saÄŸlar, bÃ¶ylece yapay zekanÄ±z her yerde eriÅŸilebilir olur.

**ğŸ“š Daha Fazla Bilgi:**
- **LLaMA-Factory GitHub**: [hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- **DokÃ¼mantasyon**: [LLaMA-Factory Belgeleri](https://llamafactory.readthedocs.io/)
- **Web ArayÃ¼zÃ¼ Demosu**: [LLaMA-Factory WebUI EÄŸitimi](https://github.com/hiyouga/LLaMA-Factory/wiki/Web-UI)
- **Model DesteÄŸi**: [Desteklenen Modeller Listesi](https://github.com/hiyouga/LLaMA-Factory#supported-models)

---

## BÃ¶lÃ¼m 7: DeepSeek AnÄ± - Hangzhouâ€™da Bir Åok DalgasÄ±

### Ezilenin Zaferi

Ocak 2025â€™te, Ã‡inâ€™in Hangzhou ÅŸehrinde bir fÄ±rtÄ±na koptu. Liang Wenfengâ€™in High-Flyer hedge fonu tarafÄ±ndan desteklenen nispeten bilinmeyen bir startup olan DeepSeek, DeepSeek-R1â€™i piyasaya sÃ¼rdÃ¼. Bu sadece baÅŸka bir dil modeli deÄŸildiâ€”bir devrimdi. ABD ihracat kontrolleriyle sÄ±nÄ±rlÄ± Nvidia H800 Ã§ipleriyle inÅŸa edilen R1, sadece 5.6 milyon dolarla OpenAIâ€™nin o1 modelinin akÄ±l yÃ¼rÃ¼tme gÃ¼cÃ¼ne ulaÅŸtÄ±; BatÄ±lÄ± devlerin 100 milyon dolarlÄ±k bÃ¼tÃ§elerinin bir kÄ±smÄ±yla.

**Teknik BÃ¼yÃ¼cÃ¼lÃ¼k**

DeepSeekâ€™in sÄ±rrÄ±, Uzmanlar KarÄ±ÅŸÄ±mÄ± (MoE) mimarisindeydi; her sorgu iÃ§in modelin sadece en ilgili kÄ±sÄ±mlarÄ±nÄ± etkinleÅŸtiren akÄ±llÄ± bir numara, hesaplama taleplerini keskin bir ÅŸekilde azalttÄ±. Ã‡ok BaÅŸlÄ± Gizli Dikkat (MLA) ile birleÅŸtirildiÄŸinde, karmaÅŸÄ±k verileri cerrahi hassasiyetle iÅŸledi. Ä°ÅŸte MoEâ€™nin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±na dair bir bakÄ±ÅŸ:

```python
class MixtureOfExperts:
    def __init__(self, uzmanlar):
        self.uzmanlar = uzmanlar  # UzmanlaÅŸmÄ±ÅŸ alt modeller listesi
        self.kapÄ± = KapÄ±AÄŸÄ±()  # Hangi uzmanÄ± kullanacaÄŸÄ±na karar verir

    def ileri(self, giriÅŸ):
        # KapÄ±, giriÅŸ iÃ§in en ilgili uzmanlarÄ± seÃ§er
        uzman_aÄŸÄ±rlÄ±klarÄ± = self.kapÄ±(giriÅŸ)
        Ã§Ä±ktÄ± = 0
        for uzman, aÄŸÄ±rlÄ±k in zip(self.uzmanlar, uzman_aÄŸÄ±rlÄ±klarÄ±):
            Ã§Ä±ktÄ± += aÄŸÄ±rlÄ±k * uzman(giriÅŸ)
        return Ã§Ä±ktÄ±
```

PekiÅŸtirme Ã¶ÄŸrenimi, aÄŸÄ±r denetimli ince ayar yerine R1â€™in matematik ve kodlama becerilerini keskinleÅŸtirdi, onu gÃ¼Ã§lÃ¼ bir rakip haline getirdi. MIT LisansÄ± altÄ±nda mevcut olan R1, dÃ¼nya Ã§apÄ±ndaki geliÅŸtiricilere GMKtec K11 gibi mÃ¼tevazÄ± donanÄ±mlarda son teknoloji modelleri ince ayar yapma gÃ¼cÃ¼ verdi.

**Wall Street Depremi**

GerÃ§ek drama, 27 Ocak 2025â€™te, DeepSeekâ€™in sohbet botunun Appleâ€™Ä±n ABD App Storeâ€™unda ChatGPTâ€™yi geÃ§erek zirveye yerleÅŸmesiyleå±•å¼€äº†ã€‚Wall Street sarsÄ±ldÄ±: Nasdaq %3,1 dÃ¼ÅŸtÃ¼, Nvidia tek bir gÃ¼nde 600 milyar dolar piyasa deÄŸeri kaybettiâ€”ABD tarihindeki en bÃ¼yÃ¼k tek gÃ¼nlÃ¼k dÃ¼ÅŸÃ¼ÅŸ. Yapay zekanÄ±n enerji aÃ§lÄ±ÄŸÄ± Ã¼zerine bahis oynayan Constellation Energy ve Vistra gibi kamu hizmeti hisseleri %20â€™den fazla deÄŸer kaybetti. Marc Andreessen bunu bir â€œSputnik anÄ±â€ olarak adlandÄ±rdÄ± ve BaÅŸkan Trump, Amerikan teknolojisi iÃ§in bir â€œuyarÄ± ziliâ€ olduÄŸunu sÃ¶yledi.

Ancak kaosun ortasÄ±nda, DeepSeekâ€™in aÃ§Ä±k kaynak ruhu kÃ¼resel bir rÃ¶nesansÄ± ateÅŸledi. R1â€™in koduyla silahlanan giriÅŸimler ve hobi sahipleri, finansal danÄ±ÅŸmanlardan tÄ±bbi teÅŸhislere kadar Ã¶zel yapay zeka Ã§Ã¶zÃ¼mleri Ã¼retmeye baÅŸladÄ±, hepsi tÃ¼ketici sÄ±nÄ±fÄ± donanÄ±mlarda Ã§alÄ±ÅŸÄ±yordu.

> **ğŸ¯ EÄŸitim BaÄŸlantÄ±sÄ±**: DeepSeek-R1â€™i `05-examples/` modÃ¼lÃ¼mÃ¼zde keÅŸfedin; burada K11â€™de LLaMA-Factory kullanarak ince ayar yapabilirsiniz. MoE verimliliÄŸini `06-advanced-techniques/` iÃ§inde Mistral Large 2 ile karÅŸÄ±laÅŸtÄ±rÄ±n ve gerÃ§ek dÃ¼nya uygulamalarÄ± iÃ§in `03-ollama/` ile yerel olarak daÄŸÄ±tÄ±n.

**ğŸ“š Daha Fazla Bilgi:**
- **DeepSeek Duyurusu**: [DeepSeek-R1 SÃ¼rÃ¼mÃ¼](https://www.deepseek.com/)
- **MoE Genel BakÄ±ÅŸ**: [Uzmanlar KarÄ±ÅŸÄ±mÄ± AÃ§Ä±klamasÄ±](https://huggingface.co/blog/moe)
- **Piyasa Etkisi**: [Bloomberg: DeepSeekâ€™in Piyasa Åoku](https://www.bloomberg.com/news/articles/2025-01-28/ai-startup-deepseek-shakes-up-market)

---

## SonsÃ¶z: Gelecek Sizin Ellerinizde

### BugÃ¼n Neredeyiz

2025â€™in ortasÄ±nda dururken ve 2026â€™ya bakarken, ince ayar devrimi yapay zekada mÃ¼mkÃ¼n olanÄ± temelden deÄŸiÅŸtirdi. Huâ€™nun temel LoRAâ€™sÄ±ndan DeepSeekâ€™in Ã§Ä±ÄŸÄ±r aÃ§an R1â€™ine kadar bu atÄ±lÄ±mlar, yapay zekanÄ±n demokratikleÅŸmesini temsil ediyor. GMKtec K11â€™iniz, GPU gÃ¶revleri iÃ§in 8GB veya daha fazla paylaÅŸÄ±lan sistem belleÄŸi ile ÅŸimdi ÅŸunlarÄ± yapabilir:
- GPT-3 ile rekabet eden modelleri ince ayar yapma
- Genel modelleri geride bÄ±rakan alan Ã¶zelinde yapay zeka eÄŸitimi
- Aylar Ã¶nce var olmayan teknikleri deney yapma
- BÃ¼yÃ¼k bÃ¼tÃ§eler olmadan hizalanmÄ±ÅŸ, yardÄ±msever yapay zeka yaratma

### Ã–ÄŸrenme YolculuÄŸu Ã–nde

**EÄŸitmenler Ä°Ã§in: Devrimi Ã–ÄŸretmek**

Bu kavramlarÄ± Ã¶ÄŸrettiÄŸinizde, sadece teknikleri aÃ§Ä±klamÄ±yorsunuzâ€”geleceÄŸin anahtarlarÄ±nÄ± paylaÅŸÄ±yorsunuz. Bu becerileri Ã¶ÄŸrenen her Ã¶ÄŸrenci, daha Ã¶nce doktora araÅŸtÄ±rmacÄ± ekipleri gerektiren sorunlarÄ± Ã§Ã¶zebilecek hale gelir.

**Dersleriniz Ä°Ã§in ÃœÃ§ Perdelik YapÄ±:**

**Birinci Perde: Sorun** (Ä°mkÃ¢nsÄ±zlÄ±kla onlarÄ± yakalayÄ±n)
- â€œChatGPTâ€™yi Ã¶zelleÅŸtirmek istediÄŸinizi hayal edin, ama eÄŸitim 12 milyon dolara mal oluyor...â€
- Ä°lerlemeyi engelleyen duvarÄ± gÃ¶sterin

**Ä°kinci Perde: AtÄ±lÄ±m** (DehayÄ± ortaya Ã§Ä±karÄ±n)
- Her keÅŸfi anlatÄ±n: LoRA, DoRA, PiSSA, RLAIF, DeepSeekâ€™in MoEâ€™si
- Benzetmeler kullanÄ±n: heykeltÄ±raÅŸlar, senfoniler, simyacÄ±lar
- Somut sayÄ±lar gÃ¶sterin: +3.7 puan, %95 bellek tasarrufu, 5.6 milyon dolarlÄ±k eÄŸitim

**ÃœÃ§Ã¼ncÃ¼ Perde: Gelecek** (OnlarÄ± katkÄ±da bulunmaya teÅŸvik edin)
- â€œSÄ°Z hangi atÄ±lÄ±mÄ± keÅŸfedeceksiniz?â€
- â€œBu hikayenin bir sonraki bÃ¶lÃ¼mÃ¼ yazÄ±lmadÄ±â€
- â€œAraÃ§lar ÅŸimdi sizin elinizdeâ€

### **EtkileÅŸimli Ã–ÄŸretim Teknikleri**

**HeykeltÄ±raÅŸ AlÄ±ÅŸtÄ±rmasÄ±**
Ã–ÄŸrencilere gerÃ§ek kil verin. BazÄ±larÄ± kaba araÃ§lar (LoRA), bazÄ±larÄ± hassas aletler (DoRA), bazÄ±larÄ± DeepSeekâ€™in MoE verimliliÄŸi kullansÄ±n. Hangi heykeller daha iyi olur? Neden?

**Bellek Oyunu**
Mistral Large 2 veya DeepSeek-R1â€™i yÃ¼klerken RAM kullanÄ±mÄ±nÄ± gerÃ§ek zamanlÄ± gÃ¶sterin. `04-quantization/` ile belleÄŸin 32GBâ€™dan 8GBâ€™a dÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ izleyin.

**Performans YarÄ±ÅŸÄ±**
LoRA, DoRA ve DeepSeek-R1 ile aynÄ± ince ayar gÃ¶revlerini Ã§alÄ±ÅŸtÄ±rÄ±n `06-advanced-techniques/` iÃ§inde. Ã–ÄŸrenciler, doÄŸruluÄŸun gerÃ§ek zamanlÄ± yÃ¼kseldiÄŸini gÃ¶rerek geleceÄŸin geÃ§miÅŸi geÃ§tiÄŸine tanÄ±k olur.

> **ğŸ¯ Tam EÄŸitim Entegrasyonu**: Bu hikayede her teknik, Ã¶ÄŸrenme modÃ¼llerimizde uygulamalÄ± uygulamalara sahiptir. Ã–ÄŸrenciler bu atÄ±lÄ±mlarÄ± sadece Ã¶ÄŸrenmezâ€”onlarÄ± yeniden yaratÄ±r, geliÅŸtirir ve kendi yeniliklerini keÅŸfeder. GerÃ§ek dÃ¼nya kullanÄ±m durumlarÄ±nÄ± gÃ¶rmek iÃ§in `05-examples/` keÅŸfedin.

### Bir Sonraki BÃ¶lÃ¼m: 2026â€™da Neler Geliyor

2026â€™ya bakarken, DeepSeek anÄ± yapay zeka kurallarÄ±nÄ± yeniden yazdÄ±. Ortaya Ã§Ä±kan trendler hikayemizi daha da heyecanlÄ± kÄ±lÄ±yor:

**Ã‡ok Modlu Devrim**
Åimdi 2025â€™te, dil modellerini deÄŸil, gÃ¶rme + dil sistemlerini birlikte uyarlÄ±yorsunuz. Kod yazabilen ve UI ekran gÃ¶rÃ¼ntÃ¼lerini anlayan bir modeli eÄŸittiÄŸinizi hayal edin. `05-examples/` modÃ¼lÃ¼mÃ¼z, K11â€™de LLaVA ile metin ve gÃ¶rÃ¼ntÃ¼ anlamayÄ± birleÅŸtiren Ã§ok modlu deneyler iÃ§erir.

**Federated Gelecek**
EÄŸitim, daÄŸÄ±tÄ±k ve Ã¶zel hale geliyor. K11â€™iniz, verileri yerel tutarken daha iyi modeller eÄŸitmek iÃ§in binlerce cihazla iÅŸbirliÄŸi yapabilir, `06-advanced-techniques/` iÃ§inde prototipler yapabileceÄŸiniz bir teknik.

**Kendi Kendini Ä°yileÅŸtiren Yapay Zeka**
Modeller, kullanÄ±cÄ± geri bildirimlerinden sÃ¼rekli olarak geliÅŸecek, hatalarÄ± otomatik dÃ¼zeltecek ve yeni alanlara manuel mÃ¼dahale olmadan uyum saÄŸlayacak. `07-system-prompt-modification/` iÃ§inde RLAIF kullanarak bu kavramlarÄ± deneyin.

**DeepSeek Dalga Etkisi**
DeepSeekâ€™in verimliliÄŸi, yapay zeka pazarÄ±nÄ± ikiye bÃ¶ldÃ¼: OpenAI gibi premium oyuncular varoluÅŸsal atÄ±lÄ±mlar peÅŸinde, DeepSeek-R2 gibi aÃ§Ä±k kaynak modeller (2026 iÃ§in planlanÄ±yor) ise kÃ¼Ã§Ã¼k iÅŸletmeleri ve giriÅŸimleri gÃ¼Ã§lendiriyor. Jevons Paradoksu, daha ucuz yapay zekanÄ±n Ã¼stel benimsenmeyi tetikleyeceÄŸini Ã¶ne sÃ¼rÃ¼yor; saÄŸlÄ±k hizmetlerinden finansa kadar Ã¶zel modelleri kÃ¶rÃ¼klÃ¼yor. ChatGPT tabanlÄ± portfÃ¶ylerin %29,22 kazanÃ§ saÄŸladÄ±ÄŸÄ± erken deneyler, yapay zeka finansal danÄ±ÅŸmanlarÄ±n insanlarÄ± geÃ§ebileceÄŸini ima ediyor. Ancak dikkat: 2025â€™te DeepSeekâ€™in sunucularÄ±na yapÄ±lan siber saldÄ±rÄ±lar zayÄ±flÄ±klarÄ± aÃ§Ä±ÄŸa Ã§Ä±kardÄ± ve Ä°talya ile Avustralyaâ€™daki yasaklar gizlilik zorluklarÄ±nÄ± iÅŸaret ediyor.

### En Ã–nemli Ders

Huâ€™nun ilk LoRA deneylerinden DeepSeekâ€™in R1â€™ine kadar bir desen beliriyor: **En derin atÄ±lÄ±mlar, genellikle baÅŸkalarÄ±nÄ±n gÃ¶z ardÄ± ettiÄŸi basit sorularÄ± sormaktan gelir.**
- Hu sordu: â€œTÃœM aÄŸÄ±rlÄ±klarÄ± gÃ¼ncellemek zorunda mÄ±yÄ±z?â€
- Liu sordu: â€œLoRA aÄŸÄ±rlÄ±k yapÄ±sÄ±nda aslÄ±nda ne yapÄ±yor?â€
- DeepSeek sordu: â€œDaha azÄ±yla devlerle eÅŸleÅŸebilir miyiz?â€

### Ã–ÄŸrencilerinizin FÄ±rsatÄ±

BugÃ¼n sÄ±nÄ±fÄ±nÄ±zda oturan Ã¶ÄŸrenciler, bu atÄ±lÄ±mlarÄ± yaratan aynÄ± araÃ§lara sahip:
- Hugging Face Ã¼zerinden **en yeni modeller** (Mistral Large 2, DeepSeek-R1)
- **GÃ¼Ã§lÃ¼ teknikler** (LoRA, DoRA, RLAIF, MoE)
- K11 gibi **eriÅŸilebilir donanÄ±m**
- Deneyleri demokratikleÅŸtiren **aÃ§Ä±k kaynak Ã§erÃ§eveler**

En Ã¶nemlisi, baÅŸkalarÄ±nÄ±n kaÃ§Ä±rdÄ±ÄŸÄ± ÅŸeyleri gÃ¶rebilecek **taze gÃ¶zlere** sahipler.

### Maceraya Ã‡aÄŸrÄ±

*â€œBu hikayede her teknik, sizin gibi birinin imkÃ¢nsÄ±z bir sorunla karÅŸÄ± karÅŸÄ±ya kalmasÄ±yla baÅŸladÄ±. Hu, GPT-3â€™Ã¼ yeniden eÄŸitmeyi gÃ¶ze alamadÄ±. DeepSeek, en iyi Ã§iplere eriÅŸemedi. Yine de dÃ¼nyayÄ± deÄŸiÅŸtirdiler.â€*

*â€œSÄ°Z hangi imkÃ¢nsÄ±z sorunu Ã§Ã¶zeceksiniz? Hangi basit soruyu soracaksÄ±nÄ±z? Bu hikayenin bir sonraki bÃ¶lÃ¼mÃ¼ sizin yazmanÄ±z iÃ§in bekliyor.â€*

---

## ğŸ“š Tam Ã–ÄŸrenme Entegrasyonu

### **Tarih Boyunca UygulamalÄ± Yolculuk**

**AtÄ±lÄ±mlar Ãœzerinden ModÃ¼l Yolu:**
- `00-first-time-beginner/`: Mistral Large 2 ile imkÃ¢nsÄ±z rÃ¼yayÄ± deneyimleyin
- `01-unsloth/`: Unslothâ€™un 2x hÄ±z artÄ±ÅŸÄ± ile eÄŸitimi hÄ±zlandÄ±rÄ±n
- `02-huggingface-peft/`: Huâ€™nun LoRA temelini Ã¶ÄŸrenin
- `03-ollama/`: Ä°nce ayar yapÄ±lmÄ±ÅŸ modelleri yerel olarak Ollama ile daÄŸÄ±tÄ±n
- `04-quantization/`: VerimliliÄŸi performansla birleÅŸtirin (QDoRA)
- `05-examples/`: DeepSeek-R1 ve Ã§ok modlu deneyler dahil gerÃ§ek dÃ¼nya uygulamalarÄ±nÄ± keÅŸfedin
- `06-advanced-techniques/`: DoRA, PiSSA ve MoE atÄ±lÄ±mlarÄ±nÄ± uygulayÄ±n
- `07-system-prompt-modification/`: Anayasal Yapay Zeka ve RLAIFâ€™i daÄŸÄ±tÄ±n
- `08-llamafactory/`: HÄ±zlÄ± deneyler iÃ§in sÄ±fÄ±r kod arayÃ¼zleri kullanÄ±n

**Kodda Tam Hikaye Kemeri:**
Ã–ÄŸrenciler yolculuÄŸu yeniden yaratÄ±r, hayal kÄ±rÄ±klÄ±klarÄ±nÄ± yaÅŸar, eureka anlarÄ±nÄ± kutlar ve bir sonraki bÃ¶lÃ¼mÃ¼ yaratmaya hazÄ±r hale gelir.

### **Tam Kaynak KÃ¼tÃ¼phanesi**
- **DoRA Makalesi**: [AÄŸÄ±rlÄ±k AyrÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ DÃ¼ÅŸÃ¼k Rank Adaptasyonu](https://arxiv.org/abs/2402.09353)
- **PiSSA Ã‡alÄ±ÅŸmalarÄ±**: [Ana Tekil DeÄŸerler Adaptasyonu](https://arxiv.org/abs/2404.02948)
- **RLAIF AraÅŸtÄ±rmasÄ±**: [Anayasal Yapay Zeka](https://arxiv.org/abs/2212.08073)
- **DeepSeek Duyurusu**: [DeepSeek-R1 SÃ¼rÃ¼mÃ¼](https://www.deepseek.com/)
- **MoE Genel BakÄ±ÅŸ**: [Uzmanlar KarÄ±ÅŸÄ±mÄ± AÃ§Ä±klamasÄ±](https://huggingface.co/blog/moe)
- **LLaMA-Factory**: [SÄ±fÄ±r kod ince ayar](https://github.com/hiyouga/LLaMA-Factory)
- **Unsloth**: [2x daha hÄ±zlÄ± eÄŸitim](https://github.com/unslothai/unsloth)
- **Ollama**: [Yerel model daÄŸÄ±tÄ±mÄ±](https://ollama.ai/)
- **AMD ROCm**: [GPU hÄ±zlandÄ±rma kÄ±lavuzu](https://rocm.docs.amd.com/)

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§ KÄ±lavuzu

### Ã–n KoÅŸullar
- **DonanÄ±m**: AMD Ryzen 9 8945HS + Radeon 780M (GPU gÃ¶revleri iÃ§in 8GB+ paylaÅŸÄ±lan sistem belleÄŸi ile)
- **RAM**: 32GB+ Ã¶nerilir
- **Depolama**: 100GB+ boÅŸ alanlÄ± hÄ±zlÄ± NVMe SSD

### Kurulum
```bash
# Bu depoyu klonlayÄ±n (not: doÄŸru yazÄ±m iÃ§in https://github.com/beyhanmeyrali/fine-tuning olarak yeniden adlandÄ±rÄ±n)
git clone https://github.com/beyhanmeyrali/fine-tunning.git
cd fine-tunning

# BaÅŸlangÄ±Ã§ modÃ¼lÃ¼yle baÅŸlayÄ±n
cd 00-first-time-beginner/
python test_setup.py  # OrtamÄ±nÄ±zÄ± doÄŸrulayÄ±n

# Veya sÄ±fÄ±r kod arayÃ¼zÃ¼ne geÃ§in
cd 08-llamafactory/
llamafactory-cli webui  # Web arayÃ¼zÃ¼nÃ¼ aÃ§ar
```

### Ã–ÄŸrenme Yolu
1. **ğŸ¯ BaÅŸlangÄ±Ã§**: `00-first-time-beginner/` (Mistral Large 2, 30 dakika)
2. **âš¡ HÄ±z**: `01-unsloth/` (2x daha hÄ±zlÄ± eÄŸitim)
3. **ğŸ”§ Standart**: `02-huggingface-peft/` (LoRA temelleri)
4. **ğŸš€ DaÄŸÄ±tÄ±m**: `03-ollama/` (Yerel model daÄŸÄ±tÄ±mÄ±)
5. **ğŸ”„ Verimlilik**: `04-quantization/` (QDoRA teknikleri)
6. **ğŸ“š Ã–rnekler**: `05-examples/` (GerÃ§ek dÃ¼nya ve DeepSeek-R1 uygulamalarÄ±)
7. **ğŸ”¬ GeliÅŸmiÅŸ**: `06-advanced-techniques/` (DoRA, PiSSA, MoE)
8. **ğŸ¤ Hizalanma**: `07-system-prompt-modification/` (RLAIF ve Anayasal Yapay Zeka)
9. **ğŸ–¥ï¸ SÄ±fÄ±r Kod**: `08-llamafactory/` (Web arayÃ¼zÃ¼)

---

## ğŸ“Š Depo Ä°statistikleri

![GitHub yÄ±ldÄ±zlarÄ±](https://img.shields.io/github/stars/beyhanmeyrali/fine-tunning?style=social)
![GitHub Ã§atallarÄ±](https://img.shields.io/github/forks/beyhanmeyrali/fine-tunning?style=social)
![GitHub sorunlarÄ±](https://img.shields.io/github/issues/beyhanmeyrali/fine-tunning)
![GitHub lisansÄ±](https://img.shields.io/github/license/beyhanmeyrali/fine-tunning)

**ğŸŒŸ Bu depoyu yÄ±ldÄ±zlayÄ±n** eÄŸer ince ayar devrimini anlamanÄ±za yardÄ±mcÄ± olduysa!

**ğŸ”„ Ã‡atallayÄ±n ve katkÄ±da bulunun** - bir sonraki atÄ±lÄ±m sizin olabilir!

---

*Bu atÄ±lÄ±mlar hikayesi, sizin keÅŸif yolculuÄŸunuz olur. Her teknik, her iÃ§gÃ¶rÃ¼, her atÄ±lÄ±m sadece akademik tarih deÄŸilâ€”usta olabileceÄŸiniz, geliÅŸtirebileceÄŸiniz ve sonunda devrim yaratabileceÄŸiniz pratik bir beceridir.*

**â¤ï¸ ile [Beyhan MEYRALI](https://www.linkedin.com/in/beyhanmeyrali/) tarafÄ±ndan oluÅŸturuldu | Yapay zekanÄ±n demokratikleÅŸmesi iÃ§in optimize edildi**