# ğŸ“ Your First AI Fine-Tuning Journey: Complete Beginner's Guide

**Created by:** [Beyhan MEYRALI](https://www.linkedin.com/in/beyhanmeyrali/)
**Optimized for:** [GMKtec K11](https://www.gmktec.com/products/amd-ryzen%E2%84%A2-9-8945hs-nucbox-k11) (AMD Ryzen 9 8945HS + Radeon 780M)
**GitHub:** https://github.com/beyhanmeyrali/fine-tunning

---

## ğŸš€ Quick Start - Get Running in 5 Minutes!

**For Complete Beginners**: Everything is already set up for you!

### âœ… What's Already Done
- âœ… Virtual environment created (`.venv/`)
- âœ… All packages installed (PyTorch, HuggingFace, etc.)
- âœ… Qwen3 0.6B model ready in Ollama
- âœ… Test scripts working
- âœ… AMD GPU optimization configured

### ğŸƒâ€â™‚ï¸ Start Fine-Tuning Right Now

#### Step 1: Enter Project Directory (30 seconds)
```bash
cd "D:\cabs\workspace\fine-tunning\00-first-time-beginner"
.venv\Scripts\activate
```

#### Step 2: Verify Setup (1 minute)
```bash
python test_setup.py
python test_qwen3.py
```
**Expected**: Green checkmarks and successful AI responses

#### Step 3: Create Training Data (5-10 minutes)
```bash
python create_dataset.py
```
**What happens**: Interactive prompt helps you create examples

#### Step 4: Train Your AI (15-30 minutes)
```bash
python train_qwen3.py
```
**What happens**: AI learns from your examples, becomes specialized

#### Step 5: Test Your Creation (2 minutes)
```bash
python test_trained_model.py
```
**What happens**: Compare before vs after performance

### ğŸ¯ Total Time: ~30-45 minutes to your first custom AI!

---

## ğŸ¤” What Exactly Are We Doing? (For Complete Beginners)

**Imagine this scenario:**
- You want a personal assistant that knows YOUR specific work style
- ChatGPT is smart but generic - it doesn't know YOUR preferences
- Building an AI from scratch = $1-12 million, 6-12 months
- **Fine-tuning** = Taking an existing smart AI and teaching it YOUR specific knowledge in 30 minutes for $2

**Real Example:**
- **Before**: "Write me a Python function" â†’ Generic code anyone could write
- **After Fine-tuning**: "Write me a Python function" â†’ Code in YOUR style, with YOUR preferred libraries, YOUR naming conventions

---

## ğŸŒ The Internet Libraries We Use (And What They Actually Do)

### **ğŸ¤— HuggingFace: The GitHub of AI Models**

**What it is**: A website where people share AI models (like GitHub for code)
**URL**: https://huggingface.co
**What we download from there**:
- The AI model's "brain" (1-2GB file)
- The "vocabulary" that teaches the AI your language

**When you run our training script, this happens:**
1. Your computer contacts HuggingFace.co
2. Downloads `Qwen/Qwen3-0.5B-Instruct` (the AI model)
3. Downloads tokenizer files (the AI's vocabulary)
4. Saves them locally so next time it's instant

**Think of it like**: Netflix for AI models - stream once, use forever

### **ğŸ“š The Python Libraries: Your AI Toolkit**

Here's what each library does and how they work together:

#### **1. PyTorch: The AI Training Engine**
```python
import torch
```
**What it does**: The core engine that makes AI training possible
**Real job**: Handles all the math (millions of calculations per second)
**Analogy**: Like the engine in a car - everything else needs this to work
**When it runs**: Every single step of training and inference

#### **2. Transformers: The AI Model Library**
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
```
**What it does**: Pre-built AI models ready to use
**What happens when you use it**:
- `AutoTokenizer`: Downloads the AI's "dictionary" from HuggingFace
- `AutoModelForCausalLM`: Downloads the actual AI "brain" from HuggingFace
**Size**: Usually 500MB - 2GB per model
**Analogy**: Like having a library of pre-trained experts

#### **3. PEFT: The Smart Training Shortcut**
```python
from peft import get_peft_model, LoraConfig
```
**What it does**: Instead of retraining the entire AI, adds small "adapter" layers
**Why brilliant**: 95% less memory, 90% faster, same results
**Analogy**: Instead of rebuilding your entire house, just add a room

#### **4. BitsAndBytes: The Memory Compressor**
```python
from transformers import BitsAndBytesConfig
```
**What it does**: Compresses AI models to use less memory
**How**: Changes from 16-bit to 4-bit precision (4x smaller)
**Quality loss**: Only 2-5% (barely noticeable)
**Why crucial**: Lets you run big models on your desktop

#### **5. TRL: The Training Helper**
```python
from trl import SFTTrainer
```
**What it does**: Handles the complex training process
**Makes easy**: Progress tracking, saving checkpoints, handling errors
**What you'd do without it**: Write 500+ lines of training code yourself

#### **6. Datasets: The Data Handler**
```python
from datasets import Dataset
```
**What it does**: Handles your training data efficiently
**Benefits**: Fast loading, memory management, format conversion
**Works with**: Your JSON files, CSV files, any data format

---

## ğŸ”— How All the Libraries Connect: Visual Guide

### **ğŸŒ The Complete Ecosystem Map**

```
                    THE INTERNET
                         |
                 HuggingFace.co
              (AI Model Repository)
                         |
                    Downloads
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â–¼         â–¼         â–¼
         Tokenizer   Model    Config
          (5MB)     (1.2GB)   (5KB)
               â”‚         â”‚         â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
                  YOUR COMPUTER
                 Local Cache Storage
               (~/.cache/huggingface/)
                         |
                    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                    â–¼         â–¼
               First Run   Later Runs
            (Download)    (Load Cache)
                    â”‚         â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â–¼
                 PYTHON LIBRARIES
                   (Your Toolkit)
```

### **ğŸ§° Library Connection Flow**

#### **Step 1: The Foundation**
```
PyTorch (The Engine)
    |
    â”œâ”€â”€ Starts AI calculations
    â”œâ”€â”€ Manages memory (CPU/GPU)
    â”œâ”€â”€ Handles all math operations
    â””â”€â”€ Required by every other library
```

#### **Step 2: Model Loading**
```
Transformers Library
    |
    â”œâ”€â”€ AutoTokenizer.from_pretrained()
    â”‚   â”œâ”€â”€ Downloads from HuggingFace
    â”‚   â”œâ”€â”€ Converts text â†” numbers
    â”‚   â””â”€â”€ Creates vocabulary mapping
    |
    â””â”€â”€ AutoModelForCausalLM.from_pretrained()
        â”œâ”€â”€ Downloads AI model
        â”œâ”€â”€ Loads into PyTorch format
        â””â”€â”€ Prepares for training
```

#### **Step 3: Memory Optimization**
```
BitsAndBytes Library
    |
    â”œâ”€â”€ BitsAndBytesConfig()
    â”‚   â”œâ”€â”€ Sets up 4-bit quantization
    â”‚   â””â”€â”€ Reduces memory by 75%
    |
    â””â”€â”€ Applied during model loading
        â”œâ”€â”€ 1.2GB model â†’ 300MB
        â””â”€â”€ Enables larger models on K11
```

#### **Step 4: Efficient Training Setup**
```
PEFT Library
    |
    â”œâ”€â”€ LoraConfig()
    â”‚   â”œâ”€â”€ Defines adapter layers
    â”‚   â”œâ”€â”€ Sets training parameters
    â”‚   â””â”€â”€ Specifies which layers to train
    |
    â””â”€â”€ get_peft_model()
        â”œâ”€â”€ Adds small adapters to model
        â”œâ”€â”€ Freezes original weights
        â””â”€â”€ Only trains new adapters (95% memory saving)
```

#### **Step 5: Data Management**
```
Datasets Library
    |
    â”œâ”€â”€ Dataset.from_list()
    â”‚   â”œâ”€â”€ Converts your JSON to AI format
    â”‚   â”œâ”€â”€ Handles memory efficiently
    â”‚   â””â”€â”€ Prepares batches for training
    |
    â””â”€â”€ Integrates with training loop
        â”œâ”€â”€ Loads data in chunks
        â””â”€â”€ Prevents memory overflow
```

#### **Step 6: Training Process**
```
TRL Library
    |
    â”œâ”€â”€ SFTTrainer()
    â”‚   â”œâ”€â”€ Manages entire training process
    â”‚   â”œâ”€â”€ Handles progress tracking
    â”‚   â”œâ”€â”€ Saves checkpoints automatically
    â”‚   â””â”€â”€ Recovers from errors
    |
    â””â”€â”€ Uses all previous components
        â”œâ”€â”€ PyTorch for calculations
        â”œâ”€â”€ Model from Transformers
        â”œâ”€â”€ Data from Datasets
        â””â”€â”€ Efficient training from PEFT
```

### **ğŸ”„ Real-Time Data Flow During Training**

```
1. Your Data (JSON file)
        â†“
2. Datasets Library (loads efficiently)
        â†“
3. Tokenizer (converts text to numbers)
        â†“
4. PEFT Model (processes through adapters)
        â†“
5. PyTorch (does calculations)
        â†“
6. TRL Trainer (manages process)
        â†“
7. BitsAndBytes (keeps memory low)
        â†“
8. Your Custom AI Model (result)
```

### **ğŸ“¦ File Storage Map**

```
Your Project Folder
â”œâ”€â”€ train_qwen.py (your script)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_dataset.json (your examples)
â”‚   â””â”€â”€ val_dataset.json (validation)
â””â”€â”€ .venv/ (all libraries installed)

System Cache
â”œâ”€â”€ C:\Users\[You]\.cache\huggingface\
â”‚   â”œâ”€â”€ models--Qwen--Qwen3-0.5B-Instruct/
â”‚   â”‚   â”œâ”€â”€ snapshots/
â”‚   â”‚   â”‚   â”œâ”€â”€ model.safetensors (1.2GB)
â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer.json (5MB)
â”‚   â”‚   â”‚   â””â”€â”€ config.json (5KB)
â”‚   â”‚   â””â”€â”€ refs/main (latest version)
â”‚   â””â”€â”€ [Other models you download...]

Training Output
â”œâ”€â”€ qwen_training_output/ (created during training)
â”‚   â”œâ”€â”€ checkpoint-100/
â”‚   â”œâ”€â”€ checkpoint-200/
â”‚   â””â”€â”€ adapter_model.safetensors (your custom adapters)
```

### **ğŸ”Œ How Libraries Talk to Each Other**

#### **Library Dependencies**
```
Your Script
    â†“
TRL (needs everything below)
    â†“
PEFT (needs PyTorch + Transformers)
    â†“
Transformers (needs PyTorch)
    â†“
PyTorch (the foundation)

BitsAndBytes (plugs into Transformers)
Datasets (plugs into TRL)
```

#### **Communication Flow**
```
train_qwen.py says: "Start training!"
    â†“
TRL says: "I need a model and data"
    â†“
Transformers says: "I'll get the model from HuggingFace"
    â†“
BitsAndBytes says: "I'll compress it to fit"
    â†“
PEFT says: "I'll add efficient adapters"
    â†“
Datasets says: "I'll handle the training data"
    â†“
PyTorch says: "I'll do all the math"
    â†“
Everyone works together: "Training complete!"
```

---

## ğŸ”„ How All These Libraries Work Together

### **Step 1: Download Phase (First Time Only)**
```
You run: python train_qwen3.py
â†“
Transformers contacts HuggingFace.co
â†“
Downloads: Qwen3-0.5B model files (1.2GB)
Downloads: Tokenizer files (5MB)
â†“
Saves to: ~/.cache/huggingface/ (for reuse)
```

### **Step 2: Loading Phase (Every Time)**
```
PyTorch: Starts up the AI engine
â†“
Transformers: Loads the AI model from local cache
â†“
BitsAndBytes: Compresses model (1.2GB â†’ 300MB)
â†“
PEFT: Adds trainable adapter layers
â†“
Ready for training!
```

### **Step 3: Training Phase**
```
Datasets: Loads your training examples
â†“
TRL: Manages the training process
â†“
PyTorch: Does millions of math calculations
â†“
PEFT: Updates only the small adapter layers
â†“
Your custom AI is ready!
```

---

## ğŸ’¾ What Gets Downloaded and Where

### **First Time Running the Script:**
```
Downloading from HuggingFace:
â”œâ”€â”€ tokenizer.json (2MB) - AI's vocabulary
â”œâ”€â”€ tokenizer_config.json (1KB) - Settings
â”œâ”€â”€ model.safetensors (1.2GB) - The AI's brain
â”œâ”€â”€ config.json (2KB) - Model configuration
â””â”€â”€ generation_config.json (1KB) - How to generate text

Total download: ~1.2GB
Saved to: C:\Users\[YourName]\.cache\huggingface\
```

### **Every Time After:**
```
Loading from local cache: Instant!
No internet needed for training
```

---

## ğŸ¯ Real Example: What Happens Step by Step

Let's trace through exactly what happens when you run `python train_qwen3.py`:

### **Minute 1: Setup & Downloads**
```
[Your computer] Importing libraries...
[PyTorch] âœ“ AI engine started
[Transformers] Checking HuggingFace for Qwen/Qwen3-0.5B-Instruct...
[Internet] Downloading model files (1.2GB)...
[Progress] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% - 2 minutes
[Local Cache] Model saved for future use
```

### **Minute 3: Model Loading**
```
[Transformers] Loading AI model from cache...
[BitsAndBytes] Compressing model (1.2GB â†’ 300MB)...
[PEFT] Adding trainable adapter layers...
[Memory] Using 500MB RAM instead of 2GB
[Status] âœ“ Ready for training
```

### **Minutes 4-30: Training**
```
[Datasets] Loading your training examples...
[TRL] Starting training process...
[PyTorch] Processing batch 1/100...
[PEFT] Updating adapter layers only...
[Progress] Training loss: 2.4 â†’ 1.8 â†’ 1.2 (getting better!)
[Status] âœ“ Training complete
```

### **Minute 30: Testing**
```
[Your AI] Testing new capabilities...
[Comparison] Before vs After results...
[Result] Your personalized AI is ready!
```

---

## ğŸ¤ How This Connects to Your Ollama Model

**The Beautiful Integration:**

1. **Training**: Use HuggingFace's `Qwen/Qwen3-0.5B-Instruct`
2. **Fine-tuning**: Create your custom version
3. **Deployment**: Convert to Ollama format
4. **Usage**: `ollama run my-custom-model`

**Why this works perfectly:**
- Same base model family (Qwen3)
- Compatible architectures
- Seamless conversion process
- Best of both worlds: HuggingFace training + Ollama deployment

---

## ğŸ“Š Cost & Time Breakdown

### **Internet Usage:**
- First download: 1.2GB (one-time)
- After that: 0MB (everything local)

### **Electricity Cost:**
- Training: ~$0.50-2.00 per run
- Your K11 power: ~120W during training

### **Time Investment:**
- First run: 35 minutes (includes download)
- Subsequent runs: 15-30 minutes (local)

### **Comparison:**
- **Cloud training**: $50-200 per run
- **Your setup**: $2 per run
- **Savings**: 95%+ cost reduction!

---

## ğŸš€ Why Your Setup is Perfect for Learning

### **GMKtec K11 Advantages:**
```
32GB RAM â†’ Handle multiple models simultaneously
8-core CPU â†’ Parallel processing during training
NVMe SSD â†’ Fast model loading and saving
AMD GPU â†’ Future compatibility with ROCm
```

### **Perfect Learning Environment:**
- **Experiment freely**: Low cost per experiment
- **Learn by doing**: See immediate results
- **Privacy**: All training happens locally
- **Scalable**: Start small, grow to larger models

---

## ğŸ¯ Why This Architecture is Brilliant

### **Modular Design Benefits**
- **Each library has one job** â†’ Easy to understand
- **Libraries can be swapped** â†’ Flexible solutions
- **Standard interfaces** â†’ Everything works together
- **Community maintained** â†’ Always improving

### **Your Learning Advantage**
- **Start simple** â†’ Add complexity gradually
- **Debug easily** â†’ Know which library handles what
- **Upgrade selectively** â†’ Update one library at a time
- **Mix and match** â†’ Combine techniques from different tutorials

---

## ğŸ”§ Troubleshooting for Beginners

### **"Where are my models stored?"**
```
Location: C:\Users\[YourName]\.cache\huggingface\hub\
Size: ~1-5GB total (grows as you download more models)
Safe to delete: Yes, will re-download when needed
```

### **"Why is the first run slow?"**
```
Reason: Downloading 1.2GB model from internet
Solution: Be patient, subsequent runs are instant
Tip: Leave computer on overnight for large downloads
```

### **"Can I use this without internet?"**
```
First run: Internet needed (downloads model)
All other runs: Completely offline
Training: 100% local, no data uploaded anywhere
```

### **"How do I know it's working?"**
```
Look for these signs:
âœ“ "Downloading model files..." (first time)
âœ“ "Loading AI model from cache..." (subsequent times)
âœ“ "Training loss decreasing..." (getting better)
âœ“ "Model saved successfully" (training complete)
```

---

## ğŸ†˜ Need Help?

**Something not working?**
1. Run: `python test_setup.py`
2. Check the output for red error messages
3. Most common fix: restart your command prompt

**Questions?**
- Check this README - covers 90% of questions
- GitHub Issues: https://github.com/beyhanmeyrali/fine-tunning/issues

---

## ğŸš€ Next Steps for Understanding

### **To Really Understand What's Happening**
1. **Run with verbose output**: Add `--verbose` to see each step
2. **Monitor memory usage**: Watch RAM/CPU during training
3. **Check the cache**: Look at what gets downloaded
4. **Experiment**: Try different models and see file sizes change

### **Advanced Understanding**
1. **Read the library docs**: Each has excellent documentation
2. **Look at the source code**: It's all open source
3. **Join the communities**: HuggingFace Discord, PyTorch forums
4. **Contribute back**: Share your own models and improvements

---

## ğŸ‰ Ready to Start?

**You now understand:**
âœ“ What each library does and why we need it
âœ“ How they all connect together
âœ“ Where files come from (HuggingFace) and where they go
âœ“ What happens behind the scenes during training
âœ“ Why your K11 is perfect for this

**Your next command:**
```bash
cd "D:\cabs\workspace\fine-tunning\00-first-time-beginner"
.venv\Scripts\activate
python train_qwen3.py
```

**Watch the magic happen** - you'll see each library doing its job, the model downloading from HuggingFace, and your personal AI being created step by step!

---

**Now you understand not just WHAT each library does, but HOW they all work together to make AI fine-tuning possible on your desktop! ğŸ‰**

**You're 5 minutes away from training your first AI model!** ğŸš€

---

## ğŸ“š Additional Step-by-Step Guides

### ğŸ› ï¸ Step 1: Complete Setup Guide

For detailed setup instructions including Ollama installation and Python environment configuration, see the sections above. The key steps are:

1. **Activate Virtual Environment**: `.venv\Scripts\activate`
2. **Verify Setup**: `python test_setup.py`
3. **Install Ollama**: Download from ollama.com
4. **Pull Model**: `ollama pull qwen3:0.6b`

### ğŸ“Š Step 2: Advanced Data Creation

Beyond the basic training concepts covered above, you can create more sophisticated datasets:

```python
# Advanced dataset creation for specific domains
def create_specialized_dataset(domain="coding"):
    """Create domain-specific training examples"""
    if domain == "coding":
        return [
            {
                "instruction": "Write a Python function to reverse a string",
                "response": "def reverse_string(s):\n    return s[::-1]"
            },
            {
                "instruction": "Explain list comprehensions in Python",
                "response": "List comprehensions provide a concise way to create lists: [expression for item in iterable if condition]"
            }
            # Add 50+ more coding examples
        ]
    # Add other domains as needed
```

### ğŸ¯ Step 3: Advanced Training Configuration

The training script above covers the basics. For advanced users who want more control:

```python
# Advanced training configuration
ADVANCED_CONFIG = {
    "learning_rate": 2e-4,           # Higher for faster learning
    "num_train_epochs": 3,           # Multiple passes through data
    "per_device_train_batch_size": 2, # Adjust based on your RAM
    "gradient_accumulation_steps": 8,  # Effective batch size = 16
    "max_seq_length": 2048,          # Longer sequences for context
    "warmup_steps": 100,             # Gradual learning rate increase
    "save_steps": 50,                # Save checkpoints frequently
    "logging_steps": 25,             # Monitor progress
}
```

---

*Remember: Every expert started exactly where you are now. The difference is understanding how the pieces fit together - and now you do!*

*This tutorial is part of the comprehensive fine-tuning learning workspace. Created with â¤ï¸ by [Beyhan MEYRALI](https://www.linkedin.com/in/beyhanmeyrali/) for extreme beginners who want to understand everything from the ground up.*